{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10856251,"sourceType":"datasetVersion","datasetId":6743222},{"sourceId":10865663,"sourceType":"datasetVersion","datasetId":6750253},{"sourceId":10868243,"sourceType":"datasetVersion","datasetId":6751838},{"sourceId":10907754,"sourceType":"datasetVersion","datasetId":6780042},{"sourceId":10912713,"sourceType":"datasetVersion","datasetId":6783639}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:27.187013Z","iopub.execute_input":"2025-03-04T02:03:27.187447Z","iopub.status.idle":"2025-03-04T02:03:27.646620Z","shell.execute_reply.started":"2025-03-04T02:03:27.187401Z","shell.execute_reply":"2025-03-04T02:03:27.645537Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/financial-data-3/data.csv\n/kaggle/input/financial-data1/withoutNaN.csv\n/kaggle/input/financial-data/withoutNaNcompany.csv\n/kaggle/input/financial-data-2/withNaN.csv\n/kaggle/input/new-data/NewLabel_WithoutNaN_filtered.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/new-data/NewLabel_WithoutNaN_filtered.csv\")\ndata=data.drop('Unnamed: 0',axis=1)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:27.648384Z","iopub.execute_input":"2025-03-04T02:03:27.649004Z","iopub.status.idle":"2025-03-04T02:03:27.727921Z","shell.execute_reply.started":"2025-03-04T02:03:27.648959Z","shell.execute_reply":"2025-03-04T02:03:27.726650Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Scode  Year  label IndcodeB         TAX           NI           AP  \\\n0      4  2010      1      C27  4434498.52  20201272.92  12974376.51   \n1      4  2011      0      C27  3196648.14   9193067.82  11914914.64   \n2      4  2012      0      C27  2893235.82   7814225.29  18392141.14   \n3      4  2013      0      C27  5406254.88   5796749.79  11538474.81   \n4      4  2014      0      C27  5040994.08  15624813.56  50865801.22   \n\n           AR            CA            CL           EQU         NCL  \\\n0  4296549.52  9.949818e+07  5.420318e+07  1.292163e+08    800000.0   \n1  4540337.99  1.097374e+08  5.709790e+07  1.384093e+08    800000.0   \n2  8129104.57  1.330801e+08  4.405773e+07  1.480558e+08    800000.0   \n3  3760743.20  1.742645e+08  7.484526e+07  1.445068e+08  22800000.0   \n4  2742071.14  2.640716e+08  1.500981e+08  1.573845e+08  30800000.0   \n\n             LB  AudOp  EmotionTone1  EmotionTone2  \n0  5.500318e+07    0.0        0.0315        0.2594  \n1  5.789790e+07    0.0        0.0311        0.2646  \n2  4.485773e+07    0.0        0.0066        0.0507  \n3  9.764526e+07    0.0        0.0074        0.0569  \n4  1.808981e+08    0.0        0.0282        0.1991  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Scode</th>\n      <th>Year</th>\n      <th>label</th>\n      <th>IndcodeB</th>\n      <th>TAX</th>\n      <th>NI</th>\n      <th>AP</th>\n      <th>AR</th>\n      <th>CA</th>\n      <th>CL</th>\n      <th>EQU</th>\n      <th>NCL</th>\n      <th>LB</th>\n      <th>AudOp</th>\n      <th>EmotionTone1</th>\n      <th>EmotionTone2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>C27</td>\n      <td>4434498.52</td>\n      <td>20201272.92</td>\n      <td>12974376.51</td>\n      <td>4296549.52</td>\n      <td>9.949818e+07</td>\n      <td>5.420318e+07</td>\n      <td>1.292163e+08</td>\n      <td>800000.0</td>\n      <td>5.500318e+07</td>\n      <td>0.0</td>\n      <td>0.0315</td>\n      <td>0.2594</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>2011</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>3196648.14</td>\n      <td>9193067.82</td>\n      <td>11914914.64</td>\n      <td>4540337.99</td>\n      <td>1.097374e+08</td>\n      <td>5.709790e+07</td>\n      <td>1.384093e+08</td>\n      <td>800000.0</td>\n      <td>5.789790e+07</td>\n      <td>0.0</td>\n      <td>0.0311</td>\n      <td>0.2646</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>2012</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>2893235.82</td>\n      <td>7814225.29</td>\n      <td>18392141.14</td>\n      <td>8129104.57</td>\n      <td>1.330801e+08</td>\n      <td>4.405773e+07</td>\n      <td>1.480558e+08</td>\n      <td>800000.0</td>\n      <td>4.485773e+07</td>\n      <td>0.0</td>\n      <td>0.0066</td>\n      <td>0.0507</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2013</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>5406254.88</td>\n      <td>5796749.79</td>\n      <td>11538474.81</td>\n      <td>3760743.20</td>\n      <td>1.742645e+08</td>\n      <td>7.484526e+07</td>\n      <td>1.445068e+08</td>\n      <td>22800000.0</td>\n      <td>9.764526e+07</td>\n      <td>0.0</td>\n      <td>0.0074</td>\n      <td>0.0569</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2014</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>5040994.08</td>\n      <td>15624813.56</td>\n      <td>50865801.22</td>\n      <td>2742071.14</td>\n      <td>2.640716e+08</td>\n      <td>1.500981e+08</td>\n      <td>1.573845e+08</td>\n      <td>30800000.0</td>\n      <td>1.808981e+08</td>\n      <td>0.0</td>\n      <td>0.0282</td>\n      <td>0.1991</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"len(data.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:27.729147Z","iopub.execute_input":"2025-03-04T02:03:27.729592Z","iopub.status.idle":"2025-03-04T02:03:27.738145Z","shell.execute_reply.started":"2025-03-04T02:03:27.729549Z","shell.execute_reply":"2025-03-04T02:03:27.735649Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"16"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"data[\"IndcodeB\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:27.739294Z","iopub.execute_input":"2025-03-04T02:03:27.739714Z","iopub.status.idle":"2025-03-04T02:03:27.758035Z","shell.execute_reply.started":"2025-03-04T02:03:27.739670Z","shell.execute_reply":"2025-03-04T02:03:27.756980Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"IndcodeB\nC39    1479\nC27    1130\nC26    1108\nC38     925\nK70     856\n       ... \nJ68       8\nO81       7\nE49       5\nR88       3\nG87       2\nName: count, Length: 76, dtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# 创建LabelEncoder对象\nlabel_encoder = LabelEncoder()\n\n# 对\"IndcodeB\"列进行编码\ndata['IndcodeB'] = label_encoder.fit_transform(data['IndcodeB'])\n\n# 查看编码后的结果\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:27.760775Z","iopub.execute_input":"2025-03-04T02:03:27.761255Z","iopub.status.idle":"2025-03-04T02:03:28.339288Z","shell.execute_reply.started":"2025-03-04T02:03:27.761209Z","shell.execute_reply":"2025-03-04T02:03:28.338001Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Scode  Year  label  IndcodeB         TAX           NI           AP  \\\n0      4  2010      1        22  4434498.52  20201272.92  12974376.51   \n1      4  2011      0        22  3196648.14   9193067.82  11914914.64   \n2      4  2012      0        22  2893235.82   7814225.29  18392141.14   \n3      4  2013      0        22  5406254.88   5796749.79  11538474.81   \n4      4  2014      0        22  5040994.08  15624813.56  50865801.22   \n\n           AR            CA            CL           EQU         NCL  \\\n0  4296549.52  9.949818e+07  5.420318e+07  1.292163e+08    800000.0   \n1  4540337.99  1.097374e+08  5.709790e+07  1.384093e+08    800000.0   \n2  8129104.57  1.330801e+08  4.405773e+07  1.480558e+08    800000.0   \n3  3760743.20  1.742645e+08  7.484526e+07  1.445068e+08  22800000.0   \n4  2742071.14  2.640716e+08  1.500981e+08  1.573845e+08  30800000.0   \n\n             LB  AudOp  EmotionTone1  EmotionTone2  \n0  5.500318e+07    0.0        0.0315        0.2594  \n1  5.789790e+07    0.0        0.0311        0.2646  \n2  4.485773e+07    0.0        0.0066        0.0507  \n3  9.764526e+07    0.0        0.0074        0.0569  \n4  1.808981e+08    0.0        0.0282        0.1991  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Scode</th>\n      <th>Year</th>\n      <th>label</th>\n      <th>IndcodeB</th>\n      <th>TAX</th>\n      <th>NI</th>\n      <th>AP</th>\n      <th>AR</th>\n      <th>CA</th>\n      <th>CL</th>\n      <th>EQU</th>\n      <th>NCL</th>\n      <th>LB</th>\n      <th>AudOp</th>\n      <th>EmotionTone1</th>\n      <th>EmotionTone2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>22</td>\n      <td>4434498.52</td>\n      <td>20201272.92</td>\n      <td>12974376.51</td>\n      <td>4296549.52</td>\n      <td>9.949818e+07</td>\n      <td>5.420318e+07</td>\n      <td>1.292163e+08</td>\n      <td>800000.0</td>\n      <td>5.500318e+07</td>\n      <td>0.0</td>\n      <td>0.0315</td>\n      <td>0.2594</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>2011</td>\n      <td>0</td>\n      <td>22</td>\n      <td>3196648.14</td>\n      <td>9193067.82</td>\n      <td>11914914.64</td>\n      <td>4540337.99</td>\n      <td>1.097374e+08</td>\n      <td>5.709790e+07</td>\n      <td>1.384093e+08</td>\n      <td>800000.0</td>\n      <td>5.789790e+07</td>\n      <td>0.0</td>\n      <td>0.0311</td>\n      <td>0.2646</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>2012</td>\n      <td>0</td>\n      <td>22</td>\n      <td>2893235.82</td>\n      <td>7814225.29</td>\n      <td>18392141.14</td>\n      <td>8129104.57</td>\n      <td>1.330801e+08</td>\n      <td>4.405773e+07</td>\n      <td>1.480558e+08</td>\n      <td>800000.0</td>\n      <td>4.485773e+07</td>\n      <td>0.0</td>\n      <td>0.0066</td>\n      <td>0.0507</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2013</td>\n      <td>0</td>\n      <td>22</td>\n      <td>5406254.88</td>\n      <td>5796749.79</td>\n      <td>11538474.81</td>\n      <td>3760743.20</td>\n      <td>1.742645e+08</td>\n      <td>7.484526e+07</td>\n      <td>1.445068e+08</td>\n      <td>22800000.0</td>\n      <td>9.764526e+07</td>\n      <td>0.0</td>\n      <td>0.0074</td>\n      <td>0.0569</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2014</td>\n      <td>0</td>\n      <td>22</td>\n      <td>5040994.08</td>\n      <td>15624813.56</td>\n      <td>50865801.22</td>\n      <td>2742071.14</td>\n      <td>2.640716e+08</td>\n      <td>1.500981e+08</td>\n      <td>1.573845e+08</td>\n      <td>30800000.0</td>\n      <td>1.808981e+08</td>\n      <td>0.0</td>\n      <td>0.0282</td>\n      <td>0.1991</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"data[\"IndcodeB\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:28.341411Z","iopub.execute_input":"2025-03-04T02:03:28.342072Z","iopub.status.idle":"2025-03-04T02:03:28.350396Z","shell.execute_reply.started":"2025-03-04T02:03:28.342028Z","shell.execute_reply":"2025-03-04T02:03:28.349261Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0        22\n1        22\n2        22\n3        22\n4        22\n         ..\n15843    38\n15844    38\n15845    38\n15846    38\n15847    38\nName: IndcodeB, Length: 15848, dtype: int64"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX,y=data.drop(columns=['label']),data[\"label\"]\nX_train, X_test, y_train, y_test = train_test_split(X,y, \n                                                    test_size=0.2,  # 测试集占20%\n                                                    random_state=42,  # 随机种子，保证结果可复现\n                                                    stratify=y)  # 按照y的比例划分","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:28.351582Z","iopub.execute_input":"2025-03-04T02:03:28.351932Z","iopub.status.idle":"2025-03-04T02:03:28.404684Z","shell.execute_reply.started":"2025-03-04T02:03:28.351904Z","shell.execute_reply":"2025-03-04T02:03:28.403443Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"y.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:28.406121Z","iopub.execute_input":"2025-03-04T02:03:28.406568Z","iopub.status.idle":"2025-03-04T02:03:28.414368Z","shell.execute_reply.started":"2025-03-04T02:03:28.406528Z","shell.execute_reply":"2025-03-04T02:03:28.413236Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"label\n0    15537\n1      311\nName: count, dtype: int64"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"15537/311","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:28.415612Z","iopub.execute_input":"2025-03-04T02:03:28.416035Z","iopub.status.idle":"2025-03-04T02:03:28.439728Z","shell.execute_reply.started":"2025-03-04T02:03:28.415993Z","shell.execute_reply":"2025-03-04T02:03:28.438638Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"49.958199356913184"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"X_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:28.440769Z","iopub.execute_input":"2025-03-04T02:03:28.441136Z","iopub.status.idle":"2025-03-04T02:03:28.485793Z","shell.execute_reply.started":"2025-03-04T02:03:28.441098Z","shell.execute_reply":"2025-03-04T02:03:28.484736Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        Scode  Year  IndcodeB           TAX            NI            AP  \\\n6801     2396  2021        34 -1.305983e+08  8.023291e+08  1.840501e+09   \n3063      927  2021        45  3.060950e+08  1.067251e+09  2.397659e+09   \n1385      608  2023        63 -1.820569e+07 -2.316339e+08  6.236031e+07   \n3836     2040  2010        51  2.172052e+06  2.791069e+07  1.805866e+07   \n8886   600010  2020        26  1.914381e+07  6.616602e+08  1.928782e+10   \n...       ...   ...       ...           ...           ...           ...   \n9904   600183  2016        34  1.285407e+08  7.606439e+08  1.741716e+09   \n12739  600575  2023        49  1.478693e+08  8.775222e+08  2.928072e+09   \n12311  600511  2015        45  1.354183e+08  5.323091e+08  1.897164e+09   \n4576     2116  2022        42  1.959706e+07  2.070915e+08  1.851564e+09   \n123        31  2021        63  2.153258e+09  7.674240e+08  1.472151e+10   \n\n                 AR            CA            CL           EQU           NCL  \\\n6801   1.758021e+09  8.502412e+09  5.283216e+09  5.652276e+09  1.874661e+08   \n3063   1.073673e+10  2.525478e+10  1.995466e+10  8.756542e+09  9.965915e+08   \n1385   1.260190e+07  1.183622e+08  6.568198e+08  2.917943e+09  7.602333e+08   \n3836   1.520956e+07  6.613974e+07  1.795330e+08  5.638807e+08  1.650194e+08   \n8886   3.232713e+09  3.913847e+10  5.735439e+10  6.161730e+10  2.525058e+10   \n...             ...           ...           ...           ...           ...   \n9904   2.954676e+09  5.313457e+09  3.679891e+09  5.304659e+09  5.493253e+08   \n12739  1.825051e+09  6.363085e+09  6.456989e+09  1.234793e+10  4.699695e+09   \n12311  2.226156e+09  5.306276e+09  2.886740e+09  3.514305e+09  1.550088e+08   \n4576   5.717937e+08  5.479269e+09  4.366015e+09  1.648398e+09  4.650696e+07   \n123    3.014772e+08  1.575624e+11  1.033025e+11  5.209118e+10  5.733339e+10   \n\n                 LB  AudOp  EmotionTone1  EmotionTone2  \n6801   5.470682e+09    0.0        0.0520        0.3673  \n3063   2.095125e+10    0.0        0.0625        0.4243  \n1385   1.417053e+09    0.0        0.0305        0.2712  \n3836   3.445524e+08    0.0        0.0432        0.4279  \n8886   8.260497e+10    0.0        0.0422        0.3111  \n...             ...    ...           ...           ...  \n9904   4.229216e+09    0.0        0.0390        0.3278  \n12739  1.115668e+10    0.0        0.0337        0.2295  \n12311  3.041749e+09    0.0        0.0502        0.3274  \n4576   4.412522e+09    0.0        0.0669        0.5191  \n123    1.606359e+11    0.0        0.0264        0.2896  \n\n[12678 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Scode</th>\n      <th>Year</th>\n      <th>IndcodeB</th>\n      <th>TAX</th>\n      <th>NI</th>\n      <th>AP</th>\n      <th>AR</th>\n      <th>CA</th>\n      <th>CL</th>\n      <th>EQU</th>\n      <th>NCL</th>\n      <th>LB</th>\n      <th>AudOp</th>\n      <th>EmotionTone1</th>\n      <th>EmotionTone2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6801</th>\n      <td>2396</td>\n      <td>2021</td>\n      <td>34</td>\n      <td>-1.305983e+08</td>\n      <td>8.023291e+08</td>\n      <td>1.840501e+09</td>\n      <td>1.758021e+09</td>\n      <td>8.502412e+09</td>\n      <td>5.283216e+09</td>\n      <td>5.652276e+09</td>\n      <td>1.874661e+08</td>\n      <td>5.470682e+09</td>\n      <td>0.0</td>\n      <td>0.0520</td>\n      <td>0.3673</td>\n    </tr>\n    <tr>\n      <th>3063</th>\n      <td>927</td>\n      <td>2021</td>\n      <td>45</td>\n      <td>3.060950e+08</td>\n      <td>1.067251e+09</td>\n      <td>2.397659e+09</td>\n      <td>1.073673e+10</td>\n      <td>2.525478e+10</td>\n      <td>1.995466e+10</td>\n      <td>8.756542e+09</td>\n      <td>9.965915e+08</td>\n      <td>2.095125e+10</td>\n      <td>0.0</td>\n      <td>0.0625</td>\n      <td>0.4243</td>\n    </tr>\n    <tr>\n      <th>1385</th>\n      <td>608</td>\n      <td>2023</td>\n      <td>63</td>\n      <td>-1.820569e+07</td>\n      <td>-2.316339e+08</td>\n      <td>6.236031e+07</td>\n      <td>1.260190e+07</td>\n      <td>1.183622e+08</td>\n      <td>6.568198e+08</td>\n      <td>2.917943e+09</td>\n      <td>7.602333e+08</td>\n      <td>1.417053e+09</td>\n      <td>0.0</td>\n      <td>0.0305</td>\n      <td>0.2712</td>\n    </tr>\n    <tr>\n      <th>3836</th>\n      <td>2040</td>\n      <td>2010</td>\n      <td>51</td>\n      <td>2.172052e+06</td>\n      <td>2.791069e+07</td>\n      <td>1.805866e+07</td>\n      <td>1.520956e+07</td>\n      <td>6.613974e+07</td>\n      <td>1.795330e+08</td>\n      <td>5.638807e+08</td>\n      <td>1.650194e+08</td>\n      <td>3.445524e+08</td>\n      <td>0.0</td>\n      <td>0.0432</td>\n      <td>0.4279</td>\n    </tr>\n    <tr>\n      <th>8886</th>\n      <td>600010</td>\n      <td>2020</td>\n      <td>26</td>\n      <td>1.914381e+07</td>\n      <td>6.616602e+08</td>\n      <td>1.928782e+10</td>\n      <td>3.232713e+09</td>\n      <td>3.913847e+10</td>\n      <td>5.735439e+10</td>\n      <td>6.161730e+10</td>\n      <td>2.525058e+10</td>\n      <td>8.260497e+10</td>\n      <td>0.0</td>\n      <td>0.0422</td>\n      <td>0.3111</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9904</th>\n      <td>600183</td>\n      <td>2016</td>\n      <td>34</td>\n      <td>1.285407e+08</td>\n      <td>7.606439e+08</td>\n      <td>1.741716e+09</td>\n      <td>2.954676e+09</td>\n      <td>5.313457e+09</td>\n      <td>3.679891e+09</td>\n      <td>5.304659e+09</td>\n      <td>5.493253e+08</td>\n      <td>4.229216e+09</td>\n      <td>0.0</td>\n      <td>0.0390</td>\n      <td>0.3278</td>\n    </tr>\n    <tr>\n      <th>12739</th>\n      <td>600575</td>\n      <td>2023</td>\n      <td>49</td>\n      <td>1.478693e+08</td>\n      <td>8.775222e+08</td>\n      <td>2.928072e+09</td>\n      <td>1.825051e+09</td>\n      <td>6.363085e+09</td>\n      <td>6.456989e+09</td>\n      <td>1.234793e+10</td>\n      <td>4.699695e+09</td>\n      <td>1.115668e+10</td>\n      <td>0.0</td>\n      <td>0.0337</td>\n      <td>0.2295</td>\n    </tr>\n    <tr>\n      <th>12311</th>\n      <td>600511</td>\n      <td>2015</td>\n      <td>45</td>\n      <td>1.354183e+08</td>\n      <td>5.323091e+08</td>\n      <td>1.897164e+09</td>\n      <td>2.226156e+09</td>\n      <td>5.306276e+09</td>\n      <td>2.886740e+09</td>\n      <td>3.514305e+09</td>\n      <td>1.550088e+08</td>\n      <td>3.041749e+09</td>\n      <td>0.0</td>\n      <td>0.0502</td>\n      <td>0.3274</td>\n    </tr>\n    <tr>\n      <th>4576</th>\n      <td>2116</td>\n      <td>2022</td>\n      <td>42</td>\n      <td>1.959706e+07</td>\n      <td>2.070915e+08</td>\n      <td>1.851564e+09</td>\n      <td>5.717937e+08</td>\n      <td>5.479269e+09</td>\n      <td>4.366015e+09</td>\n      <td>1.648398e+09</td>\n      <td>4.650696e+07</td>\n      <td>4.412522e+09</td>\n      <td>0.0</td>\n      <td>0.0669</td>\n      <td>0.5191</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>31</td>\n      <td>2021</td>\n      <td>63</td>\n      <td>2.153258e+09</td>\n      <td>7.674240e+08</td>\n      <td>1.472151e+10</td>\n      <td>3.014772e+08</td>\n      <td>1.575624e+11</td>\n      <td>1.033025e+11</td>\n      <td>5.209118e+10</td>\n      <td>5.733339e+10</td>\n      <td>1.606359e+11</td>\n      <td>0.0</td>\n      <td>0.0264</td>\n      <td>0.2896</td>\n    </tr>\n  </tbody>\n</table>\n<p>12678 rows × 15 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:28.486994Z","iopub.execute_input":"2025-03-04T02:03:28.487426Z","iopub.status.idle":"2025-03-04T02:03:28.497211Z","shell.execute_reply.started":"2025-03-04T02:03:28.487386Z","shell.execute_reply":"2025-03-04T02:03:28.495648Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"label\n0    3108\n1      62\nName: count, dtype: int64"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## 1.1 XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:28.498564Z","iopub.execute_input":"2025-03-04T02:03:28.499045Z","iopub.status.idle":"2025-03-04T02:03:28.545878Z","shell.execute_reply.started":"2025-03-04T02:03:28.499002Z","shell.execute_reply":"2025-03-04T02:03:28.544789Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:28.546863Z","iopub.execute_input":"2025-03-04T02:03:28.547166Z","iopub.status.idle":"2025-03-04T02:03:28.577129Z","shell.execute_reply.started":"2025-03-04T02:03:28.547138Z","shell.execute_reply":"2025-03-04T02:03:28.573554Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# 设置参数\nparams = {\n    'objective': 'binary:logistic',\n    'max_depth': 3,\n    'eta': 0.1,\n    'eval_metric': 'logloss'\n}\n\n# 训练模型\nnum_round = 100\nbst = xgb.train(params, dtrain, num_round, evals=[(dtest, 'eval')], early_stopping_rounds=10)\n\n# 预测与评估\npreds = bst.predict(dtest)\nbest_preds = preds > 0.5\naccuracy = (best_preds == y_test).mean()\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:28.578158Z","iopub.execute_input":"2025-03-04T02:03:28.578646Z","iopub.status.idle":"2025-03-04T02:03:29.198162Z","shell.execute_reply.started":"2025-03-04T02:03:28.578603Z","shell.execute_reply":"2025-03-04T02:03:29.197282Z"}},"outputs":[{"name":"stdout","text":"[0]\teval-logloss:0.16278\n[1]\teval-logloss:0.15287\n[2]\teval-logloss:0.14410\n[3]\teval-logloss:0.13647\n[4]\teval-logloss:0.12969\n[5]\teval-logloss:0.12360\n[6]\teval-logloss:0.11821\n[7]\teval-logloss:0.11343\n[8]\teval-logloss:0.10926\n[9]\teval-logloss:0.10556\n[10]\teval-logloss:0.10216\n[11]\teval-logloss:0.09921\n[12]\teval-logloss:0.09637\n[13]\teval-logloss:0.09389\n[14]\teval-logloss:0.09180\n[15]\teval-logloss:0.09003\n[16]\teval-logloss:0.08837\n[17]\teval-logloss:0.08694\n[18]\teval-logloss:0.08551\n[19]\teval-logloss:0.08431\n[20]\teval-logloss:0.08316\n[21]\teval-logloss:0.08211\n[22]\teval-logloss:0.08109\n[23]\teval-logloss:0.08013\n[24]\teval-logloss:0.07956\n[25]\teval-logloss:0.07880\n[26]\teval-logloss:0.07811\n[27]\teval-logloss:0.07768\n[28]\teval-logloss:0.07726\n[29]\teval-logloss:0.07695\n[30]\teval-logloss:0.07662\n[31]\teval-logloss:0.07628\n[32]\teval-logloss:0.07598\n[33]\teval-logloss:0.07573\n[34]\teval-logloss:0.07537\n[35]\teval-logloss:0.07516\n[36]\teval-logloss:0.07492\n[37]\teval-logloss:0.07486\n[38]\teval-logloss:0.07466\n[39]\teval-logloss:0.07434\n[40]\teval-logloss:0.07421\n[41]\teval-logloss:0.07410\n[42]\teval-logloss:0.07380\n[43]\teval-logloss:0.07374\n[44]\teval-logloss:0.07360\n[45]\teval-logloss:0.07359\n[46]\teval-logloss:0.07346\n[47]\teval-logloss:0.07351\n[48]\teval-logloss:0.07361\n[49]\teval-logloss:0.07349\n[50]\teval-logloss:0.07340\n[51]\teval-logloss:0.07303\n[52]\teval-logloss:0.07290\n[53]\teval-logloss:0.07287\n[54]\teval-logloss:0.07279\n[55]\teval-logloss:0.07275\n[56]\teval-logloss:0.07261\n[57]\teval-logloss:0.07267\n[58]\teval-logloss:0.07253\n[59]\teval-logloss:0.07252\n[60]\teval-logloss:0.07244\n[61]\teval-logloss:0.07224\n[62]\teval-logloss:0.07225\n[63]\teval-logloss:0.07214\n[64]\teval-logloss:0.07199\n[65]\teval-logloss:0.07191\n[66]\teval-logloss:0.07174\n[67]\teval-logloss:0.07166\n[68]\teval-logloss:0.07159\n[69]\teval-logloss:0.07171\n[70]\teval-logloss:0.07165\n[71]\teval-logloss:0.07155\n[72]\teval-logloss:0.07147\n[73]\teval-logloss:0.07141\n[74]\teval-logloss:0.07144\n[75]\teval-logloss:0.07134\n[76]\teval-logloss:0.07130\n[77]\teval-logloss:0.07125\n[78]\teval-logloss:0.07118\n[79]\teval-logloss:0.07115\n[80]\teval-logloss:0.07111\n[81]\teval-logloss:0.07106\n[82]\teval-logloss:0.07095\n[83]\teval-logloss:0.07098\n[84]\teval-logloss:0.07073\n[85]\teval-logloss:0.07081\n[86]\teval-logloss:0.07078\n[87]\teval-logloss:0.07068\n[88]\teval-logloss:0.07066\n[89]\teval-logloss:0.07075\n[90]\teval-logloss:0.07079\n[91]\teval-logloss:0.07068\n[92]\teval-logloss:0.07075\n[93]\teval-logloss:0.07075\n[94]\teval-logloss:0.07074\n[95]\teval-logloss:0.07070\n[96]\teval-logloss:0.07068\n[97]\teval-logloss:0.07074\nAccuracy: 98.08%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, brier_score_loss, precision_score, recall_score, f1_score, accuracy_score\nfrom scipy.stats import ks_2samp\n\n# 训练模型\nnum_round = 100\nbst = xgb.train(params, dtrain, num_round, evals=[(dtest, 'eval')], early_stopping_rounds=10)\n\n# 预测概率\npreds_proba = bst.predict(dtest)\n\n# 预测类别\npreds_class = (preds_proba > 0.5).astype(int)\n\n# 计算评估指标\nauc = roc_auc_score(y_test, preds_proba)\nbs = brier_score_loss(y_test, preds_proba)\n\nks, _ = ks_2samp(preds_proba[y_test == 1], preds_proba[y_test == 0])\nprecision = precision_score(y_test, preds_class)\nrecall = recall_score(y_test, preds_class)\nf1 = f1_score(y_test, preds_class)\naccuracy = accuracy_score(y_test, preds_class)\n# from sklearn.metrics import recall_score\n\n# 计算敏感性（Recall），这里直接使用recall_score即为敏感性\nsensitivity = recall\n\n# 计算特异性（Specificity）\nspecificity = recall_score(y_test, preds_class, pos_label=0)\n\n# 计算G-Mean\ngmean = (sensitivity * specificity) ** 0.5\n\n# 打印G-Mean\nprint(f\"G-Mean: {gmean:.4f}\")\n# 打印评估指标\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"Brier Score: {bs:.4f}\")\nprint(f\"KS Statistic: {ks:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:29.198840Z","iopub.execute_input":"2025-03-04T02:03:29.199090Z","iopub.status.idle":"2025-03-04T02:03:29.620897Z","shell.execute_reply.started":"2025-03-04T02:03:29.199068Z","shell.execute_reply":"2025-03-04T02:03:29.619879Z"}},"outputs":[{"name":"stdout","text":"[0]\teval-logloss:0.16278\n[1]\teval-logloss:0.15287\n[2]\teval-logloss:0.14410\n[3]\teval-logloss:0.13647\n[4]\teval-logloss:0.12969\n[5]\teval-logloss:0.12360\n[6]\teval-logloss:0.11821\n[7]\teval-logloss:0.11343\n[8]\teval-logloss:0.10926\n[9]\teval-logloss:0.10556\n[10]\teval-logloss:0.10216\n[11]\teval-logloss:0.09921\n[12]\teval-logloss:0.09637\n[13]\teval-logloss:0.09389\n[14]\teval-logloss:0.09180\n[15]\teval-logloss:0.09003\n[16]\teval-logloss:0.08837\n[17]\teval-logloss:0.08694\n[18]\teval-logloss:0.08551\n[19]\teval-logloss:0.08431\n[20]\teval-logloss:0.08316\n[21]\teval-logloss:0.08211\n[22]\teval-logloss:0.08109\n[23]\teval-logloss:0.08013\n[24]\teval-logloss:0.07956\n[25]\teval-logloss:0.07880\n[26]\teval-logloss:0.07811\n[27]\teval-logloss:0.07768\n[28]\teval-logloss:0.07726\n[29]\teval-logloss:0.07695\n[30]\teval-logloss:0.07662\n[31]\teval-logloss:0.07628\n[32]\teval-logloss:0.07598\n[33]\teval-logloss:0.07573\n[34]\teval-logloss:0.07537\n[35]\teval-logloss:0.07516\n[36]\teval-logloss:0.07492\n[37]\teval-logloss:0.07486\n[38]\teval-logloss:0.07466\n[39]\teval-logloss:0.07434\n[40]\teval-logloss:0.07421\n[41]\teval-logloss:0.07410\n[42]\teval-logloss:0.07380\n[43]\teval-logloss:0.07374\n[44]\teval-logloss:0.07360\n[45]\teval-logloss:0.07359\n[46]\teval-logloss:0.07346\n[47]\teval-logloss:0.07351\n[48]\teval-logloss:0.07361\n[49]\teval-logloss:0.07349\n[50]\teval-logloss:0.07340\n[51]\teval-logloss:0.07303\n[52]\teval-logloss:0.07290\n[53]\teval-logloss:0.07287\n[54]\teval-logloss:0.07279\n[55]\teval-logloss:0.07275\n[56]\teval-logloss:0.07261\n[57]\teval-logloss:0.07267\n[58]\teval-logloss:0.07253\n[59]\teval-logloss:0.07252\n[60]\teval-logloss:0.07244\n[61]\teval-logloss:0.07224\n[62]\teval-logloss:0.07225\n[63]\teval-logloss:0.07214\n[64]\teval-logloss:0.07199\n[65]\teval-logloss:0.07191\n[66]\teval-logloss:0.07174\n[67]\teval-logloss:0.07166\n[68]\teval-logloss:0.07159\n[69]\teval-logloss:0.07171\n[70]\teval-logloss:0.07165\n[71]\teval-logloss:0.07155\n[72]\teval-logloss:0.07147\n[73]\teval-logloss:0.07141\n[74]\teval-logloss:0.07144\n[75]\teval-logloss:0.07134\n[76]\teval-logloss:0.07130\n[77]\teval-logloss:0.07125\n[78]\teval-logloss:0.07118\n[79]\teval-logloss:0.07115\n[80]\teval-logloss:0.07111\n[81]\teval-logloss:0.07106\n[82]\teval-logloss:0.07095\n[83]\teval-logloss:0.07098\n[84]\teval-logloss:0.07073\n[85]\teval-logloss:0.07081\n[86]\teval-logloss:0.07078\n[87]\teval-logloss:0.07068\n[88]\teval-logloss:0.07066\n[89]\teval-logloss:0.07075\n[90]\teval-logloss:0.07079\n[91]\teval-logloss:0.07068\n[92]\teval-logloss:0.07075\n[93]\teval-logloss:0.07075\n[94]\teval-logloss:0.07074\n[95]\teval-logloss:0.07070\n[96]\teval-logloss:0.07068\n[97]\teval-logloss:0.07074\nG-Mean: 0.3108\nAUC: 0.8759\nBrier Score: 0.0167\nKS Statistic: 0.6249\nPrecision: 0.5455\nRecall: 0.0968\nF1 Score: 0.1644\nAccuracy: 0.9808\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# 使用np.unique找到所有唯一值及其计数\nunique_values, counts = np.unique(best_preds, return_counts=True)\n\n# 计算唯一值的数量\nunique_count = len(unique_values)\n\nprint(\"唯一值的数量:\", unique_count)\nprint(\"每个唯一值的计数:\")\nfor value, count in zip(unique_values, counts):\n    print(f\"值 {value}: 出现了 {count} 次\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:29.621975Z","iopub.execute_input":"2025-03-04T02:03:29.622325Z","iopub.status.idle":"2025-03-04T02:03:29.630077Z","shell.execute_reply.started":"2025-03-04T02:03:29.622292Z","shell.execute_reply":"2025-03-04T02:03:29.628784Z"}},"outputs":[{"name":"stdout","text":"唯一值的数量: 2\n每个唯一值的计数:\n值 False: 出现了 3159 次\n值 True: 出现了 11 次\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.2 CATBoost","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:56.394883Z","iopub.execute_input":"2025-03-04T02:03:56.395252Z","iopub.status.idle":"2025-03-04T02:03:56.571591Z","shell.execute_reply.started":"2025-03-04T02:03:56.395224Z","shell.execute_reply":"2025-03-04T02:03:56.570401Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/new-data/NewLabel_WithoutNaN_filtered.csv\")\ndata.drop(\"Unnamed: 0\",axis=1,inplace=True)\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:56.572894Z","iopub.execute_input":"2025-03-04T02:03:56.573179Z","iopub.status.idle":"2025-03-04T02:03:56.645644Z","shell.execute_reply.started":"2025-03-04T02:03:56.573154Z","shell.execute_reply":"2025-03-04T02:03:56.644737Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"        Scode  Year  label IndcodeB           TAX            NI            AP  \\\n0           4  2010      1      C27  4.434499e+06  2.020127e+07  1.297438e+07   \n1           4  2011      0      C27  3.196648e+06  9.193068e+06  1.191491e+07   \n2           4  2012      0      C27  2.893236e+06  7.814225e+06  1.839214e+07   \n3           4  2013      0      C27  5.406255e+06  5.796750e+06  1.153847e+07   \n4           4  2014      0      C27  5.040994e+06  1.562481e+07  5.086580e+07   \n...       ...   ...    ...      ...           ...           ...           ...   \n15843  601991  2019      0      D44  1.710728e+09  2.977266e+09  1.767167e+10   \n15844  601991  2020      0      D44  1.888606e+09  5.316037e+09  1.742278e+10   \n15845  601991  2021      0      D44  6.272780e+08 -1.189567e+10  2.590976e+10   \n15846  601991  2022      0      D44  8.517970e+08 -8.714150e+08  2.658625e+10   \n15847  601991  2023      0      D44  2.566171e+09  3.004926e+09  2.179735e+10   \n\n                 AR            CA            CL           EQU           NCL  \\\n0      4.296550e+06  9.949818e+07  5.420318e+07  1.292163e+08  8.000000e+05   \n1      4.540338e+06  1.097374e+08  5.709790e+07  1.384093e+08  8.000000e+05   \n2      8.129105e+06  1.330801e+08  4.405773e+07  1.480558e+08  8.000000e+05   \n3      3.760743e+06  1.742645e+08  7.484526e+07  1.445068e+08  2.280000e+07   \n4      2.742071e+06  2.640716e+08  1.500981e+08  1.573845e+08  3.080000e+07   \n...             ...           ...           ...           ...           ...   \n15843  1.478491e+10  3.357388e+10  8.035384e+10  8.175278e+10  1.200137e+11   \n15844  1.514736e+10  3.371234e+10  7.902023e+10  9.139256e+10  1.099207e+11   \n15845  1.835950e+10  4.563710e+10  9.602209e+10  7.616704e+10  1.237786e+11   \n15846  1.805911e+10  4.308924e+10  1.024289e+11  7.631169e+10  1.263118e+11   \n15847  1.961905e+10  4.029522e+10  9.123032e+10  8.846595e+10  1.243026e+11   \n\n                 LB  AudOp  EmotionTone1  EmotionTone2  \n0      5.500318e+07    0.0        0.0315        0.2594  \n1      5.789790e+07    0.0        0.0311        0.2646  \n2      4.485773e+07    0.0        0.0066        0.0507  \n3      9.764526e+07    0.0        0.0074        0.0569  \n4      1.808981e+08    0.0        0.0282        0.1991  \n...             ...    ...           ...           ...  \n15843  2.003675e+11    0.0        0.0263        0.2162  \n15844  1.889410e+11    0.0        0.0348        0.2933  \n15845  2.198007e+11    0.0        0.0425        0.3206  \n15846  2.287408e+11    0.0        0.0354        0.3119  \n15847  2.155330e+11    0.0        0.0289        0.2688  \n\n[15848 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Scode</th>\n      <th>Year</th>\n      <th>label</th>\n      <th>IndcodeB</th>\n      <th>TAX</th>\n      <th>NI</th>\n      <th>AP</th>\n      <th>AR</th>\n      <th>CA</th>\n      <th>CL</th>\n      <th>EQU</th>\n      <th>NCL</th>\n      <th>LB</th>\n      <th>AudOp</th>\n      <th>EmotionTone1</th>\n      <th>EmotionTone2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>C27</td>\n      <td>4.434499e+06</td>\n      <td>2.020127e+07</td>\n      <td>1.297438e+07</td>\n      <td>4.296550e+06</td>\n      <td>9.949818e+07</td>\n      <td>5.420318e+07</td>\n      <td>1.292163e+08</td>\n      <td>8.000000e+05</td>\n      <td>5.500318e+07</td>\n      <td>0.0</td>\n      <td>0.0315</td>\n      <td>0.2594</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>2011</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>3.196648e+06</td>\n      <td>9.193068e+06</td>\n      <td>1.191491e+07</td>\n      <td>4.540338e+06</td>\n      <td>1.097374e+08</td>\n      <td>5.709790e+07</td>\n      <td>1.384093e+08</td>\n      <td>8.000000e+05</td>\n      <td>5.789790e+07</td>\n      <td>0.0</td>\n      <td>0.0311</td>\n      <td>0.2646</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>2012</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>2.893236e+06</td>\n      <td>7.814225e+06</td>\n      <td>1.839214e+07</td>\n      <td>8.129105e+06</td>\n      <td>1.330801e+08</td>\n      <td>4.405773e+07</td>\n      <td>1.480558e+08</td>\n      <td>8.000000e+05</td>\n      <td>4.485773e+07</td>\n      <td>0.0</td>\n      <td>0.0066</td>\n      <td>0.0507</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2013</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>5.406255e+06</td>\n      <td>5.796750e+06</td>\n      <td>1.153847e+07</td>\n      <td>3.760743e+06</td>\n      <td>1.742645e+08</td>\n      <td>7.484526e+07</td>\n      <td>1.445068e+08</td>\n      <td>2.280000e+07</td>\n      <td>9.764526e+07</td>\n      <td>0.0</td>\n      <td>0.0074</td>\n      <td>0.0569</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2014</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>5.040994e+06</td>\n      <td>1.562481e+07</td>\n      <td>5.086580e+07</td>\n      <td>2.742071e+06</td>\n      <td>2.640716e+08</td>\n      <td>1.500981e+08</td>\n      <td>1.573845e+08</td>\n      <td>3.080000e+07</td>\n      <td>1.808981e+08</td>\n      <td>0.0</td>\n      <td>0.0282</td>\n      <td>0.1991</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15843</th>\n      <td>601991</td>\n      <td>2019</td>\n      <td>0</td>\n      <td>D44</td>\n      <td>1.710728e+09</td>\n      <td>2.977266e+09</td>\n      <td>1.767167e+10</td>\n      <td>1.478491e+10</td>\n      <td>3.357388e+10</td>\n      <td>8.035384e+10</td>\n      <td>8.175278e+10</td>\n      <td>1.200137e+11</td>\n      <td>2.003675e+11</td>\n      <td>0.0</td>\n      <td>0.0263</td>\n      <td>0.2162</td>\n    </tr>\n    <tr>\n      <th>15844</th>\n      <td>601991</td>\n      <td>2020</td>\n      <td>0</td>\n      <td>D44</td>\n      <td>1.888606e+09</td>\n      <td>5.316037e+09</td>\n      <td>1.742278e+10</td>\n      <td>1.514736e+10</td>\n      <td>3.371234e+10</td>\n      <td>7.902023e+10</td>\n      <td>9.139256e+10</td>\n      <td>1.099207e+11</td>\n      <td>1.889410e+11</td>\n      <td>0.0</td>\n      <td>0.0348</td>\n      <td>0.2933</td>\n    </tr>\n    <tr>\n      <th>15845</th>\n      <td>601991</td>\n      <td>2021</td>\n      <td>0</td>\n      <td>D44</td>\n      <td>6.272780e+08</td>\n      <td>-1.189567e+10</td>\n      <td>2.590976e+10</td>\n      <td>1.835950e+10</td>\n      <td>4.563710e+10</td>\n      <td>9.602209e+10</td>\n      <td>7.616704e+10</td>\n      <td>1.237786e+11</td>\n      <td>2.198007e+11</td>\n      <td>0.0</td>\n      <td>0.0425</td>\n      <td>0.3206</td>\n    </tr>\n    <tr>\n      <th>15846</th>\n      <td>601991</td>\n      <td>2022</td>\n      <td>0</td>\n      <td>D44</td>\n      <td>8.517970e+08</td>\n      <td>-8.714150e+08</td>\n      <td>2.658625e+10</td>\n      <td>1.805911e+10</td>\n      <td>4.308924e+10</td>\n      <td>1.024289e+11</td>\n      <td>7.631169e+10</td>\n      <td>1.263118e+11</td>\n      <td>2.287408e+11</td>\n      <td>0.0</td>\n      <td>0.0354</td>\n      <td>0.3119</td>\n    </tr>\n    <tr>\n      <th>15847</th>\n      <td>601991</td>\n      <td>2023</td>\n      <td>0</td>\n      <td>D44</td>\n      <td>2.566171e+09</td>\n      <td>3.004926e+09</td>\n      <td>2.179735e+10</td>\n      <td>1.961905e+10</td>\n      <td>4.029522e+10</td>\n      <td>9.123032e+10</td>\n      <td>8.846595e+10</td>\n      <td>1.243026e+11</td>\n      <td>2.155330e+11</td>\n      <td>0.0</td>\n      <td>0.0289</td>\n      <td>0.2688</td>\n    </tr>\n  </tbody>\n</table>\n<p>15848 rows × 16 columns</p>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"X,y=data.drop(columns=['label']),data[\"label\"]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, \n                                                    test_size=0.2,  # 测试集占20%\n                                                    random_state=42,  # 随机种子，保证结果可复现\n                                                    stratify=y)  # 按照y的比例划分","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:56.712478Z","iopub.execute_input":"2025-03-04T02:03:56.712893Z","iopub.status.idle":"2025-03-04T02:03:56.731052Z","shell.execute_reply.started":"2025-03-04T02:03:56.712860Z","shell.execute_reply":"2025-03-04T02:03:56.729804Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, brier_score_loss\nfrom scipy.stats import ks_2samp\nimport numpy as np\n\n# 假设X_train, X_test, y_train, y_test已经定义\n\n# 指定哪些特征是类别型的（在这个例子中，IndcodeB是）\ncat_features_indices = [\"IndcodeB\"]\n\n# 创建CatBoost数据池\ntrain_pool = Pool(data=X_train, label=y_train, cat_features=cat_features_indices)\ntest_pool = Pool(data=X_test, label=y_test, cat_features=cat_features_indices)\n\n# 设置CatBoost参数\nparams = {\n    'iterations': 100,\n    'learning_rate': 0.1,\n    'depth': 6,\n    'loss_function': 'Logloss',  # 对于二分类问题\n    'eval_metric': 'AUC',\n    'random_seed': 42,\n    'verbose': 10  # 每10次迭代打印一次日志\n}\n\n# 训练模型\nmodel = CatBoostClassifier(**params)\nmodel.fit(train_pool, eval_set=test_pool, early_stopping_rounds=20)\n\n# 在测试集上进行预测\npreds_proba = model.predict_proba(test_pool)[:, 1]  # 获取正类的概率\npreds_class = model.predict(test_pool)  # 获取预测类别\n\n# 计算评估指标\nauc = roc_auc_score(y_test, preds_proba)\nbrier = brier_score_loss(y_test, preds_proba)\nks_stat, _ = ks_2samp(preds_proba[y_test == 1], preds_proba[y_test == 0])\nprecision = precision_score(y_test, preds_class)\nrecall = recall_score(y_test, preds_class)\nf1 = f1_score(y_test, preds_class)\naccuracy = accuracy_score(y_test, preds_class)\n\n# 计算G-Mean\nsensitivity = recall_score(y_test, preds_class, pos_label=1)  # 灵敏度（召回率）\nspecificity = recall_score(y_test, preds_class, pos_label=0)  # 特异度\ngmean = np.sqrt(sensitivity * specificity)\n\n# 打印所有评估指标\nprint(f\"G-Mean: {gmean:.4f}\")\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"Brier Score: {brier:.4f}\")\nprint(f\"KS Statistic: {ks_stat:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:03:58.398316Z","iopub.execute_input":"2025-03-04T02:03:58.398723Z","iopub.status.idle":"2025-03-04T02:03:59.082854Z","shell.execute_reply.started":"2025-03-04T02:03:58.398688Z","shell.execute_reply":"2025-03-04T02:03:59.081700Z"}},"outputs":[{"name":"stdout","text":"0:\ttest: 0.7402152\tbest: 0.7402152 (0)\ttotal: 56ms\tremaining: 5.54s\n10:\ttest: 0.8098326\tbest: 0.8145940 (9)\ttotal: 110ms\tremaining: 886ms\n20:\ttest: 0.8256892\tbest: 0.8285123 (19)\ttotal: 159ms\tremaining: 597ms\n30:\ttest: 0.8307022\tbest: 0.8307022 (30)\ttotal: 210ms\tremaining: 467ms\n40:\ttest: 0.8340443\tbest: 0.8350407 (34)\ttotal: 264ms\tremaining: 380ms\n50:\ttest: 0.8413460\tbest: 0.8416729 (49)\ttotal: 314ms\tremaining: 302ms\n60:\ttest: 0.8407751\tbest: 0.8416729 (49)\ttotal: 363ms\tremaining: 232ms\n70:\ttest: 0.8438836\tbest: 0.8438836 (70)\ttotal: 414ms\tremaining: 169ms\n80:\ttest: 0.8465614\tbest: 0.8465614 (80)\ttotal: 466ms\tremaining: 109ms\n90:\ttest: 0.8492444\tbest: 0.8492444 (90)\ttotal: 517ms\tremaining: 51.1ms\n99:\ttest: 0.8504328\tbest: 0.8504328 (99)\ttotal: 564ms\tremaining: 0us\n\nbestTest = 0.8504328061\nbestIteration = 99\n\nG-Mean: 0.1795\nAUC: 0.8504\nBrier Score: 0.0171\nKS Statistic: 0.5692\nPrecision: 0.5000\nRecall: 0.0323\nF1 Score: 0.0606\nAccuracy: 0.9804\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"\n# 使用np.unique找到所有唯一值及其计数\nunique_values, counts = np.unique(preds_class, return_counts=True)\n\n# 计算唯一值的数量\nunique_count = len(unique_values)\n\nprint(\"唯一值的数量:\", unique_count)\nprint(\"每个唯一值的计数:\")\nfor value, count in zip(unique_values, counts):\n    print(f\"值 {value}: 出现了 {count} 次\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:04:10.993090Z","iopub.execute_input":"2025-03-04T02:04:10.993464Z","iopub.status.idle":"2025-03-04T02:04:11.000767Z","shell.execute_reply.started":"2025-03-04T02:04:10.993435Z","shell.execute_reply":"2025-03-04T02:04:10.999511Z"}},"outputs":[{"name":"stdout","text":"唯一值的数量: 2\n每个唯一值的计数:\n值 0: 出现了 3166 次\n值 1: 出现了 4 次\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:04:12.864368Z","iopub.execute_input":"2025-03-04T02:04:12.864806Z","iopub.status.idle":"2025-03-04T02:04:12.875343Z","shell.execute_reply.started":"2025-03-04T02:04:12.864771Z","shell.execute_reply":"2025-03-04T02:04:12.873355Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"label\n0    3108\n1      62\nName: count, dtype: int64"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"## 1.3 NN","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# 创建LabelEncoder对象\nlabel_encoder = LabelEncoder()\n\n# 对\"IndcodeB\"列进行编码\ndata['IndcodeB'] = label_encoder.fit_transform(data['IndcodeB'])\n\n# 查看编码后的结果\ndata.head()\n\nX,y=data.drop(columns=['label']),data[\"label\"]\n# 数据预处理\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# 构建模型\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))  # 输入层+第一隐藏层\nmodel.add(Dense(16, activation='relu'))  # 第二隐藏层\nmodel.add(Dense(1, activation='sigmoid'))  # 输出层，如果是多分类请使用softmax\n\n# 编译模型\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# 训练模型\nhistory = model.fit(X_train, y_train, epochs=30, batch_size=10, validation_data=(X_test, y_test))\n\n# 评估模型\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:04:19.958564Z","iopub.execute_input":"2025-03-04T02:04:19.958963Z","iopub.status.idle":"2025-03-04T02:05:32.275087Z","shell.execute_reply.started":"2025-03-04T02:04:19.958933Z","shell.execute_reply":"2025-03-04T02:05:32.274016Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9616 - loss: 0.1696 - val_accuracy: 0.9767 - val_loss: 0.0963\nEpoch 2/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0842 - val_accuracy: 0.9767 - val_loss: 0.0966\nEpoch 3/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0834 - val_accuracy: 0.9767 - val_loss: 0.0920\nEpoch 4/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.0807 - val_accuracy: 0.9767 - val_loss: 0.0919\nEpoch 5/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0812 - val_accuracy: 0.9767 - val_loss: 0.0958\nEpoch 6/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9815 - loss: 0.0783 - val_accuracy: 0.9767 - val_loss: 0.0933\nEpoch 7/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0762 - val_accuracy: 0.9767 - val_loss: 0.0934\nEpoch 8/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9822 - loss: 0.0726 - val_accuracy: 0.9767 - val_loss: 0.0921\nEpoch 9/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9824 - loss: 0.0703 - val_accuracy: 0.9767 - val_loss: 0.0930\nEpoch 10/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0708 - val_accuracy: 0.9767 - val_loss: 0.0922\nEpoch 11/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0734 - val_accuracy: 0.9767 - val_loss: 0.0915\nEpoch 12/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9809 - loss: 0.0718 - val_accuracy: 0.9767 - val_loss: 0.0923\nEpoch 13/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0707 - val_accuracy: 0.9767 - val_loss: 0.0924\nEpoch 14/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0729 - val_accuracy: 0.9773 - val_loss: 0.0935\nEpoch 15/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0693 - val_accuracy: 0.9767 - val_loss: 0.0890\nEpoch 16/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0671 - val_accuracy: 0.9767 - val_loss: 0.0884\nEpoch 17/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9814 - loss: 0.0715 - val_accuracy: 0.9767 - val_loss: 0.0901\nEpoch 18/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0664 - val_accuracy: 0.9770 - val_loss: 0.0910\nEpoch 19/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0705 - val_accuracy: 0.9770 - val_loss: 0.0930\nEpoch 20/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0730 - val_accuracy: 0.9767 - val_loss: 0.0952\nEpoch 21/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0694 - val_accuracy: 0.9773 - val_loss: 0.0894\nEpoch 22/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0704 - val_accuracy: 0.9770 - val_loss: 0.0935\nEpoch 23/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9827 - loss: 0.0630 - val_accuracy: 0.9773 - val_loss: 0.0880\nEpoch 24/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0641 - val_accuracy: 0.9773 - val_loss: 0.0900\nEpoch 25/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9821 - loss: 0.0696 - val_accuracy: 0.9763 - val_loss: 0.0885\nEpoch 26/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9816 - loss: 0.0655 - val_accuracy: 0.9770 - val_loss: 0.0919\nEpoch 27/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0643 - val_accuracy: 0.9773 - val_loss: 0.0943\nEpoch 28/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.0693 - val_accuracy: 0.9770 - val_loss: 0.0905\nEpoch 29/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9830 - loss: 0.0645 - val_accuracy: 0.9767 - val_loss: 0.0927\nEpoch 30/30\n\u001b[1m1268/1268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9838 - loss: 0.0635 - val_accuracy: 0.9773 - val_loss: 0.0922\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9792 - loss: 0.0854\nTest Accuracy: 0.98\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# 在测试集上进行预测\npreds_proba = model.predict(X_test)  # 获取正类的概率\npreds_class = (model.predict(X_test) > 0.5).astype(\"int32\")  # 获取预测类别\n\n# 计算评估指标\nauc = roc_auc_score(y_test, preds_proba)\nbrier = brier_score_loss(y_test, preds_proba)\nks_stat, _ = ks_2samp(preds_proba[y_test == 1].ravel(), preds_proba[y_test == 0].ravel())\nprecision = precision_score(y_test, preds_class)\nrecall = recall_score(y_test, preds_class)\nf1 = f1_score(y_test, preds_class)\naccuracy = accuracy_score(y_test, preds_class)\n\n# 计算G-Mean\nsensitivity = recall_score(y_test, preds_class, pos_label=1)  # 灵敏度（召回率）\nspecificity = recall_score(y_test, preds_class, pos_label=0)  # 特异度\ngmean = np.sqrt(sensitivity * specificity)\n\n# 打印所有评估指标\nprint(f\"G-Mean: {gmean:.4f}\")\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"Brier Score: {brier:.4f}\")\nprint(f\"KS Statistic: {ks_stat:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:32.276526Z","iopub.execute_input":"2025-03-04T02:05:32.277215Z","iopub.status.idle":"2025-03-04T02:05:32.806576Z","shell.execute_reply.started":"2025-03-04T02:05:32.277179Z","shell.execute_reply":"2025-03-04T02:05:32.805590Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\nG-Mean: 0.1644\nAUC: 0.8233\nBrier Score: 0.0203\nKS Statistic: 0.5440\nPrecision: 1.0000\nRecall: 0.0270\nF1 Score: 0.0526\nAccuracy: 0.9773\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## 2.1 XGBoost+SMote","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/new-data/NewLabel_WithoutNaN_filtered.csv\")\ndata.drop(\"Unnamed: 0\",axis=1,inplace=True)\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:32.808598Z","iopub.execute_input":"2025-03-04T02:05:32.809011Z","iopub.status.idle":"2025-03-04T02:05:32.884117Z","shell.execute_reply.started":"2025-03-04T02:05:32.808968Z","shell.execute_reply":"2025-03-04T02:05:32.882898Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"        Scode  Year  label IndcodeB           TAX            NI            AP  \\\n0           4  2010      1      C27  4.434499e+06  2.020127e+07  1.297438e+07   \n1           4  2011      0      C27  3.196648e+06  9.193068e+06  1.191491e+07   \n2           4  2012      0      C27  2.893236e+06  7.814225e+06  1.839214e+07   \n3           4  2013      0      C27  5.406255e+06  5.796750e+06  1.153847e+07   \n4           4  2014      0      C27  5.040994e+06  1.562481e+07  5.086580e+07   \n...       ...   ...    ...      ...           ...           ...           ...   \n15843  601991  2019      0      D44  1.710728e+09  2.977266e+09  1.767167e+10   \n15844  601991  2020      0      D44  1.888606e+09  5.316037e+09  1.742278e+10   \n15845  601991  2021      0      D44  6.272780e+08 -1.189567e+10  2.590976e+10   \n15846  601991  2022      0      D44  8.517970e+08 -8.714150e+08  2.658625e+10   \n15847  601991  2023      0      D44  2.566171e+09  3.004926e+09  2.179735e+10   \n\n                 AR            CA            CL           EQU           NCL  \\\n0      4.296550e+06  9.949818e+07  5.420318e+07  1.292163e+08  8.000000e+05   \n1      4.540338e+06  1.097374e+08  5.709790e+07  1.384093e+08  8.000000e+05   \n2      8.129105e+06  1.330801e+08  4.405773e+07  1.480558e+08  8.000000e+05   \n3      3.760743e+06  1.742645e+08  7.484526e+07  1.445068e+08  2.280000e+07   \n4      2.742071e+06  2.640716e+08  1.500981e+08  1.573845e+08  3.080000e+07   \n...             ...           ...           ...           ...           ...   \n15843  1.478491e+10  3.357388e+10  8.035384e+10  8.175278e+10  1.200137e+11   \n15844  1.514736e+10  3.371234e+10  7.902023e+10  9.139256e+10  1.099207e+11   \n15845  1.835950e+10  4.563710e+10  9.602209e+10  7.616704e+10  1.237786e+11   \n15846  1.805911e+10  4.308924e+10  1.024289e+11  7.631169e+10  1.263118e+11   \n15847  1.961905e+10  4.029522e+10  9.123032e+10  8.846595e+10  1.243026e+11   \n\n                 LB  AudOp  EmotionTone1  EmotionTone2  \n0      5.500318e+07    0.0        0.0315        0.2594  \n1      5.789790e+07    0.0        0.0311        0.2646  \n2      4.485773e+07    0.0        0.0066        0.0507  \n3      9.764526e+07    0.0        0.0074        0.0569  \n4      1.808981e+08    0.0        0.0282        0.1991  \n...             ...    ...           ...           ...  \n15843  2.003675e+11    0.0        0.0263        0.2162  \n15844  1.889410e+11    0.0        0.0348        0.2933  \n15845  2.198007e+11    0.0        0.0425        0.3206  \n15846  2.287408e+11    0.0        0.0354        0.3119  \n15847  2.155330e+11    0.0        0.0289        0.2688  \n\n[15848 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Scode</th>\n      <th>Year</th>\n      <th>label</th>\n      <th>IndcodeB</th>\n      <th>TAX</th>\n      <th>NI</th>\n      <th>AP</th>\n      <th>AR</th>\n      <th>CA</th>\n      <th>CL</th>\n      <th>EQU</th>\n      <th>NCL</th>\n      <th>LB</th>\n      <th>AudOp</th>\n      <th>EmotionTone1</th>\n      <th>EmotionTone2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>C27</td>\n      <td>4.434499e+06</td>\n      <td>2.020127e+07</td>\n      <td>1.297438e+07</td>\n      <td>4.296550e+06</td>\n      <td>9.949818e+07</td>\n      <td>5.420318e+07</td>\n      <td>1.292163e+08</td>\n      <td>8.000000e+05</td>\n      <td>5.500318e+07</td>\n      <td>0.0</td>\n      <td>0.0315</td>\n      <td>0.2594</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>2011</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>3.196648e+06</td>\n      <td>9.193068e+06</td>\n      <td>1.191491e+07</td>\n      <td>4.540338e+06</td>\n      <td>1.097374e+08</td>\n      <td>5.709790e+07</td>\n      <td>1.384093e+08</td>\n      <td>8.000000e+05</td>\n      <td>5.789790e+07</td>\n      <td>0.0</td>\n      <td>0.0311</td>\n      <td>0.2646</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>2012</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>2.893236e+06</td>\n      <td>7.814225e+06</td>\n      <td>1.839214e+07</td>\n      <td>8.129105e+06</td>\n      <td>1.330801e+08</td>\n      <td>4.405773e+07</td>\n      <td>1.480558e+08</td>\n      <td>8.000000e+05</td>\n      <td>4.485773e+07</td>\n      <td>0.0</td>\n      <td>0.0066</td>\n      <td>0.0507</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2013</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>5.406255e+06</td>\n      <td>5.796750e+06</td>\n      <td>1.153847e+07</td>\n      <td>3.760743e+06</td>\n      <td>1.742645e+08</td>\n      <td>7.484526e+07</td>\n      <td>1.445068e+08</td>\n      <td>2.280000e+07</td>\n      <td>9.764526e+07</td>\n      <td>0.0</td>\n      <td>0.0074</td>\n      <td>0.0569</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2014</td>\n      <td>0</td>\n      <td>C27</td>\n      <td>5.040994e+06</td>\n      <td>1.562481e+07</td>\n      <td>5.086580e+07</td>\n      <td>2.742071e+06</td>\n      <td>2.640716e+08</td>\n      <td>1.500981e+08</td>\n      <td>1.573845e+08</td>\n      <td>3.080000e+07</td>\n      <td>1.808981e+08</td>\n      <td>0.0</td>\n      <td>0.0282</td>\n      <td>0.1991</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15843</th>\n      <td>601991</td>\n      <td>2019</td>\n      <td>0</td>\n      <td>D44</td>\n      <td>1.710728e+09</td>\n      <td>2.977266e+09</td>\n      <td>1.767167e+10</td>\n      <td>1.478491e+10</td>\n      <td>3.357388e+10</td>\n      <td>8.035384e+10</td>\n      <td>8.175278e+10</td>\n      <td>1.200137e+11</td>\n      <td>2.003675e+11</td>\n      <td>0.0</td>\n      <td>0.0263</td>\n      <td>0.2162</td>\n    </tr>\n    <tr>\n      <th>15844</th>\n      <td>601991</td>\n      <td>2020</td>\n      <td>0</td>\n      <td>D44</td>\n      <td>1.888606e+09</td>\n      <td>5.316037e+09</td>\n      <td>1.742278e+10</td>\n      <td>1.514736e+10</td>\n      <td>3.371234e+10</td>\n      <td>7.902023e+10</td>\n      <td>9.139256e+10</td>\n      <td>1.099207e+11</td>\n      <td>1.889410e+11</td>\n      <td>0.0</td>\n      <td>0.0348</td>\n      <td>0.2933</td>\n    </tr>\n    <tr>\n      <th>15845</th>\n      <td>601991</td>\n      <td>2021</td>\n      <td>0</td>\n      <td>D44</td>\n      <td>6.272780e+08</td>\n      <td>-1.189567e+10</td>\n      <td>2.590976e+10</td>\n      <td>1.835950e+10</td>\n      <td>4.563710e+10</td>\n      <td>9.602209e+10</td>\n      <td>7.616704e+10</td>\n      <td>1.237786e+11</td>\n      <td>2.198007e+11</td>\n      <td>0.0</td>\n      <td>0.0425</td>\n      <td>0.3206</td>\n    </tr>\n    <tr>\n      <th>15846</th>\n      <td>601991</td>\n      <td>2022</td>\n      <td>0</td>\n      <td>D44</td>\n      <td>8.517970e+08</td>\n      <td>-8.714150e+08</td>\n      <td>2.658625e+10</td>\n      <td>1.805911e+10</td>\n      <td>4.308924e+10</td>\n      <td>1.024289e+11</td>\n      <td>7.631169e+10</td>\n      <td>1.263118e+11</td>\n      <td>2.287408e+11</td>\n      <td>0.0</td>\n      <td>0.0354</td>\n      <td>0.3119</td>\n    </tr>\n    <tr>\n      <th>15847</th>\n      <td>601991</td>\n      <td>2023</td>\n      <td>0</td>\n      <td>D44</td>\n      <td>2.566171e+09</td>\n      <td>3.004926e+09</td>\n      <td>2.179735e+10</td>\n      <td>1.961905e+10</td>\n      <td>4.029522e+10</td>\n      <td>9.123032e+10</td>\n      <td>8.846595e+10</td>\n      <td>1.243026e+11</td>\n      <td>2.155330e+11</td>\n      <td>0.0</td>\n      <td>0.0289</td>\n      <td>0.2688</td>\n    </tr>\n  </tbody>\n</table>\n<p>15848 rows × 16 columns</p>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, brier_score_loss\nfrom scipy.stats import ks_2samp\nfrom imblearn.over_sampling import SMOTE\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\n# 创建LabelEncoder对象\nlabel_encoder = LabelEncoder()\n\n# 对\"IndcodeB\"列进行编码\ndata['IndcodeB'] = label_encoder.fit_transform(data['IndcodeB'])\n\n# 查看编码后的结果\ndata.head()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:32.885654Z","iopub.execute_input":"2025-03-04T02:05:32.885975Z","iopub.status.idle":"2025-03-04T02:05:33.016628Z","shell.execute_reply.started":"2025-03-04T02:05:32.885941Z","shell.execute_reply":"2025-03-04T02:05:33.015549Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"   Scode  Year  label  IndcodeB         TAX           NI           AP  \\\n0      4  2010      1        22  4434498.52  20201272.92  12974376.51   \n1      4  2011      0        22  3196648.14   9193067.82  11914914.64   \n2      4  2012      0        22  2893235.82   7814225.29  18392141.14   \n3      4  2013      0        22  5406254.88   5796749.79  11538474.81   \n4      4  2014      0        22  5040994.08  15624813.56  50865801.22   \n\n           AR            CA            CL           EQU         NCL  \\\n0  4296549.52  9.949818e+07  5.420318e+07  1.292163e+08    800000.0   \n1  4540337.99  1.097374e+08  5.709790e+07  1.384093e+08    800000.0   \n2  8129104.57  1.330801e+08  4.405773e+07  1.480558e+08    800000.0   \n3  3760743.20  1.742645e+08  7.484526e+07  1.445068e+08  22800000.0   \n4  2742071.14  2.640716e+08  1.500981e+08  1.573845e+08  30800000.0   \n\n             LB  AudOp  EmotionTone1  EmotionTone2  \n0  5.500318e+07    0.0        0.0315        0.2594  \n1  5.789790e+07    0.0        0.0311        0.2646  \n2  4.485773e+07    0.0        0.0066        0.0507  \n3  9.764526e+07    0.0        0.0074        0.0569  \n4  1.808981e+08    0.0        0.0282        0.1991  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Scode</th>\n      <th>Year</th>\n      <th>label</th>\n      <th>IndcodeB</th>\n      <th>TAX</th>\n      <th>NI</th>\n      <th>AP</th>\n      <th>AR</th>\n      <th>CA</th>\n      <th>CL</th>\n      <th>EQU</th>\n      <th>NCL</th>\n      <th>LB</th>\n      <th>AudOp</th>\n      <th>EmotionTone1</th>\n      <th>EmotionTone2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>22</td>\n      <td>4434498.52</td>\n      <td>20201272.92</td>\n      <td>12974376.51</td>\n      <td>4296549.52</td>\n      <td>9.949818e+07</td>\n      <td>5.420318e+07</td>\n      <td>1.292163e+08</td>\n      <td>800000.0</td>\n      <td>5.500318e+07</td>\n      <td>0.0</td>\n      <td>0.0315</td>\n      <td>0.2594</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>2011</td>\n      <td>0</td>\n      <td>22</td>\n      <td>3196648.14</td>\n      <td>9193067.82</td>\n      <td>11914914.64</td>\n      <td>4540337.99</td>\n      <td>1.097374e+08</td>\n      <td>5.709790e+07</td>\n      <td>1.384093e+08</td>\n      <td>800000.0</td>\n      <td>5.789790e+07</td>\n      <td>0.0</td>\n      <td>0.0311</td>\n      <td>0.2646</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>2012</td>\n      <td>0</td>\n      <td>22</td>\n      <td>2893235.82</td>\n      <td>7814225.29</td>\n      <td>18392141.14</td>\n      <td>8129104.57</td>\n      <td>1.330801e+08</td>\n      <td>4.405773e+07</td>\n      <td>1.480558e+08</td>\n      <td>800000.0</td>\n      <td>4.485773e+07</td>\n      <td>0.0</td>\n      <td>0.0066</td>\n      <td>0.0507</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2013</td>\n      <td>0</td>\n      <td>22</td>\n      <td>5406254.88</td>\n      <td>5796749.79</td>\n      <td>11538474.81</td>\n      <td>3760743.20</td>\n      <td>1.742645e+08</td>\n      <td>7.484526e+07</td>\n      <td>1.445068e+08</td>\n      <td>22800000.0</td>\n      <td>9.764526e+07</td>\n      <td>0.0</td>\n      <td>0.0074</td>\n      <td>0.0569</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2014</td>\n      <td>0</td>\n      <td>22</td>\n      <td>5040994.08</td>\n      <td>15624813.56</td>\n      <td>50865801.22</td>\n      <td>2742071.14</td>\n      <td>2.640716e+08</td>\n      <td>1.500981e+08</td>\n      <td>1.573845e+08</td>\n      <td>30800000.0</td>\n      <td>1.808981e+08</td>\n      <td>0.0</td>\n      <td>0.0282</td>\n      <td>0.1991</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX,y=data.drop(columns=['label']),data[\"label\"]\nX_train, X_test, y_train, y_test = train_test_split(X,y, \n                                                    test_size=0.2,  # 测试集占20%\n                                                    random_state=42,  # 随机种子，保证结果可复现\n                                                    stratify=y)  # 按照y的比例划分","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:33.017683Z","iopub.execute_input":"2025-03-04T02:05:33.018421Z","iopub.status.idle":"2025-03-04T02:05:33.037327Z","shell.execute_reply.started":"2025-03-04T02:05:33.018378Z","shell.execute_reply":"2025-03-04T02:05:33.036030Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"\n\n# 使用SMOTE对训练集进行过采样\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\nX_train_resampled\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:33.038139Z","iopub.execute_input":"2025-03-04T02:05:33.038485Z","iopub.status.idle":"2025-03-04T02:05:33.094647Z","shell.execute_reply.started":"2025-03-04T02:05:33.038455Z","shell.execute_reply":"2025-03-04T02:05:33.093632Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"        Scode  Year  IndcodeB           TAX            NI            AP  \\\n0        2396  2021        34 -1.305983e+08  8.023291e+08  1.840501e+09   \n1         927  2021        45  3.060950e+08  1.067251e+09  2.397659e+09   \n2         608  2023        63 -1.820569e+07 -2.316339e+08  6.236031e+07   \n3        2040  2010        51  2.172052e+06  2.791069e+07  1.805866e+07   \n4      600010  2020        26  1.914381e+07  6.616602e+08  1.928782e+10   \n...       ...   ...       ...           ...           ...           ...   \n24853  600649  2013        15  8.505210e+06  4.695166e+07  2.683944e+07   \n24854    1846  2017        27  1.385023e+06  1.193035e+07  8.277044e+07   \n24855    1822  2020        42 -2.163949e+09 -1.388335e+10  1.540654e+10   \n24856  138925  2018        42  1.239611e+08  7.558936e+08  4.050087e+08   \n24857  442973  2014        22  1.526348e+06 -3.731567e+08  2.095332e+08   \n\n                 AR            CA            CL           EQU           NCL  \\\n0      1.758021e+09  8.502412e+09  5.283216e+09  5.652276e+09  1.874661e+08   \n1      1.073673e+10  2.525478e+10  1.995466e+10  8.756542e+09  9.965915e+08   \n2      1.260190e+07  1.183622e+08  6.568198e+08  2.917943e+09  7.602333e+08   \n3      1.520956e+07  6.613974e+07  1.795330e+08  5.638807e+08  1.650194e+08   \n4      3.232713e+09  3.913847e+10  5.735439e+10  6.161730e+10  2.525058e+10   \n...             ...           ...           ...           ...           ...   \n24853  3.087543e+07  3.766523e+08  3.159759e+08  3.940750e+08  6.383523e+07   \n24854  1.240974e+08  3.143746e+08  2.082875e+08  4.401777e+08  1.204538e+07   \n24855  2.721991e+09  5.161558e+10  9.121586e+10  1.371279e+10  1.868402e+10   \n24856  5.140333e+08  4.288688e+09  2.595445e+09  2.080209e+09  3.455832e+08   \n24857  4.070418e+08  1.007726e+09  7.456568e+08  1.195537e+09  3.543086e+07   \n\n                 LB     AudOp  EmotionTone1  EmotionTone2  \n0      5.470682e+09  0.000000      0.052000      0.367300  \n1      2.095125e+10  0.000000      0.062500      0.424300  \n2      1.417053e+09  0.000000      0.030500      0.271200  \n3      3.445524e+08  0.000000      0.043200      0.427900  \n4      8.260497e+10  0.000000      0.042200      0.311100  \n...             ...       ...           ...           ...  \n24853  3.798111e+08  0.000000      0.015291      0.095207  \n24854  2.203328e+08  0.247646      0.037810      0.267976  \n24855  1.098999e+11  0.815000      0.023089      0.194345  \n24856  2.941029e+09  0.000000      0.047076      0.382331  \n24857  7.810876e+08  0.736436      0.024020      0.156259  \n\n[24858 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Scode</th>\n      <th>Year</th>\n      <th>IndcodeB</th>\n      <th>TAX</th>\n      <th>NI</th>\n      <th>AP</th>\n      <th>AR</th>\n      <th>CA</th>\n      <th>CL</th>\n      <th>EQU</th>\n      <th>NCL</th>\n      <th>LB</th>\n      <th>AudOp</th>\n      <th>EmotionTone1</th>\n      <th>EmotionTone2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2396</td>\n      <td>2021</td>\n      <td>34</td>\n      <td>-1.305983e+08</td>\n      <td>8.023291e+08</td>\n      <td>1.840501e+09</td>\n      <td>1.758021e+09</td>\n      <td>8.502412e+09</td>\n      <td>5.283216e+09</td>\n      <td>5.652276e+09</td>\n      <td>1.874661e+08</td>\n      <td>5.470682e+09</td>\n      <td>0.000000</td>\n      <td>0.052000</td>\n      <td>0.367300</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>927</td>\n      <td>2021</td>\n      <td>45</td>\n      <td>3.060950e+08</td>\n      <td>1.067251e+09</td>\n      <td>2.397659e+09</td>\n      <td>1.073673e+10</td>\n      <td>2.525478e+10</td>\n      <td>1.995466e+10</td>\n      <td>8.756542e+09</td>\n      <td>9.965915e+08</td>\n      <td>2.095125e+10</td>\n      <td>0.000000</td>\n      <td>0.062500</td>\n      <td>0.424300</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>608</td>\n      <td>2023</td>\n      <td>63</td>\n      <td>-1.820569e+07</td>\n      <td>-2.316339e+08</td>\n      <td>6.236031e+07</td>\n      <td>1.260190e+07</td>\n      <td>1.183622e+08</td>\n      <td>6.568198e+08</td>\n      <td>2.917943e+09</td>\n      <td>7.602333e+08</td>\n      <td>1.417053e+09</td>\n      <td>0.000000</td>\n      <td>0.030500</td>\n      <td>0.271200</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2040</td>\n      <td>2010</td>\n      <td>51</td>\n      <td>2.172052e+06</td>\n      <td>2.791069e+07</td>\n      <td>1.805866e+07</td>\n      <td>1.520956e+07</td>\n      <td>6.613974e+07</td>\n      <td>1.795330e+08</td>\n      <td>5.638807e+08</td>\n      <td>1.650194e+08</td>\n      <td>3.445524e+08</td>\n      <td>0.000000</td>\n      <td>0.043200</td>\n      <td>0.427900</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>600010</td>\n      <td>2020</td>\n      <td>26</td>\n      <td>1.914381e+07</td>\n      <td>6.616602e+08</td>\n      <td>1.928782e+10</td>\n      <td>3.232713e+09</td>\n      <td>3.913847e+10</td>\n      <td>5.735439e+10</td>\n      <td>6.161730e+10</td>\n      <td>2.525058e+10</td>\n      <td>8.260497e+10</td>\n      <td>0.000000</td>\n      <td>0.042200</td>\n      <td>0.311100</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24853</th>\n      <td>600649</td>\n      <td>2013</td>\n      <td>15</td>\n      <td>8.505210e+06</td>\n      <td>4.695166e+07</td>\n      <td>2.683944e+07</td>\n      <td>3.087543e+07</td>\n      <td>3.766523e+08</td>\n      <td>3.159759e+08</td>\n      <td>3.940750e+08</td>\n      <td>6.383523e+07</td>\n      <td>3.798111e+08</td>\n      <td>0.000000</td>\n      <td>0.015291</td>\n      <td>0.095207</td>\n    </tr>\n    <tr>\n      <th>24854</th>\n      <td>1846</td>\n      <td>2017</td>\n      <td>27</td>\n      <td>1.385023e+06</td>\n      <td>1.193035e+07</td>\n      <td>8.277044e+07</td>\n      <td>1.240974e+08</td>\n      <td>3.143746e+08</td>\n      <td>2.082875e+08</td>\n      <td>4.401777e+08</td>\n      <td>1.204538e+07</td>\n      <td>2.203328e+08</td>\n      <td>0.247646</td>\n      <td>0.037810</td>\n      <td>0.267976</td>\n    </tr>\n    <tr>\n      <th>24855</th>\n      <td>1822</td>\n      <td>2020</td>\n      <td>42</td>\n      <td>-2.163949e+09</td>\n      <td>-1.388335e+10</td>\n      <td>1.540654e+10</td>\n      <td>2.721991e+09</td>\n      <td>5.161558e+10</td>\n      <td>9.121586e+10</td>\n      <td>1.371279e+10</td>\n      <td>1.868402e+10</td>\n      <td>1.098999e+11</td>\n      <td>0.815000</td>\n      <td>0.023089</td>\n      <td>0.194345</td>\n    </tr>\n    <tr>\n      <th>24856</th>\n      <td>138925</td>\n      <td>2018</td>\n      <td>42</td>\n      <td>1.239611e+08</td>\n      <td>7.558936e+08</td>\n      <td>4.050087e+08</td>\n      <td>5.140333e+08</td>\n      <td>4.288688e+09</td>\n      <td>2.595445e+09</td>\n      <td>2.080209e+09</td>\n      <td>3.455832e+08</td>\n      <td>2.941029e+09</td>\n      <td>0.000000</td>\n      <td>0.047076</td>\n      <td>0.382331</td>\n    </tr>\n    <tr>\n      <th>24857</th>\n      <td>442973</td>\n      <td>2014</td>\n      <td>22</td>\n      <td>1.526348e+06</td>\n      <td>-3.731567e+08</td>\n      <td>2.095332e+08</td>\n      <td>4.070418e+08</td>\n      <td>1.007726e+09</td>\n      <td>7.456568e+08</td>\n      <td>1.195537e+09</td>\n      <td>3.543086e+07</td>\n      <td>7.810876e+08</td>\n      <td>0.736436</td>\n      <td>0.024020</td>\n      <td>0.156259</td>\n    </tr>\n  </tbody>\n</table>\n<p>24858 rows × 15 columns</p>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X_train_resampled, label=y_train_resampled)\ndtest = xgb.DMatrix(X_test, label=y_test)\n# 设置参数\nparams = {\n    'objective': 'binary:logistic',\n    'max_depth': 3,\n    'eta': 0.1,\n    'eval_metric': 'logloss'\n}\n\n# 训练模型\nnum_round = 100\nbst = xgb.train(params, dtrain, num_round, evals=[(dtest, 'eval')], early_stopping_rounds=10)\n\n# 预测与评估\npreds = bst.predict(dtest)\nbest_preds = preds > 0.5\naccuracy = (best_preds == y_test).mean()\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nfrom sklearn.metrics import roc_auc_score, brier_score_loss, precision_score, recall_score, f1_score, accuracy_score\nfrom scipy.stats import ks_2samp\n\n# 训练模型\nnum_round = 100\nbst = xgb.train(params, dtrain, num_round, evals=[(dtest, 'eval')], early_stopping_rounds=10)\n\n# 预测概率\npreds_proba = bst.predict(dtest)\n\n# 预测类别\npreds_class = (preds_proba > 0.5).astype(int)\n\n# 计算评估指标\nauc = roc_auc_score(y_test, preds_proba)\nbs = brier_score_loss(y_test, preds_proba)\n\nks, _ = ks_2samp(preds_proba[y_test == 1], preds_proba[y_test == 0])\nprecision = precision_score(y_test, preds_class)\nrecall = recall_score(y_test, preds_class)\nf1 = f1_score(y_test, preds_class)\naccuracy = accuracy_score(y_test, preds_class)\n# from sklearn.metrics import recall_score\n\n# 计算敏感性（Recall），这里直接使用recall_score即为敏感性\nsensitivity = recall\n\n# 计算特异性（Specificity）\nspecificity = recall_score(y_test, preds_class, pos_label=0)\n\n# 计算G-Mean\ngmean = (sensitivity * specificity) ** 0.5\n\n# 打印G-Mean\nprint(f\"G-Mean: {gmean:.4f}\")\n# 打印评估指标\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"Brier Score: {bs:.4f}\")\nprint(f\"KS Statistic: {ks:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:33.095593Z","iopub.execute_input":"2025-03-04T02:05:33.095946Z","iopub.status.idle":"2025-03-04T02:05:34.009807Z","shell.execute_reply.started":"2025-03-04T02:05:33.095918Z","shell.execute_reply":"2025-03-04T02:05:34.008053Z"}},"outputs":[{"name":"stdout","text":"[0]\teval-logloss:0.65352\n[1]\teval-logloss:0.61931\n[2]\teval-logloss:0.58949\n[3]\teval-logloss:0.56532\n[4]\teval-logloss:0.54421\n[5]\teval-logloss:0.52497\n[6]\teval-logloss:0.50753\n[7]\teval-logloss:0.49291\n[8]\teval-logloss:0.48070\n[9]\teval-logloss:0.46819\n[10]\teval-logloss:0.45718\n[11]\teval-logloss:0.44707\n[12]\teval-logloss:0.43841\n[13]\teval-logloss:0.43072\n[14]\teval-logloss:0.42409\n[15]\teval-logloss:0.41174\n[16]\teval-logloss:0.40552\n[17]\teval-logloss:0.39958\n[18]\teval-logloss:0.38980\n[19]\teval-logloss:0.38484\n[20]\teval-logloss:0.37636\n[21]\teval-logloss:0.37205\n[22]\teval-logloss:0.36725\n[23]\teval-logloss:0.36487\n[24]\teval-logloss:0.36148\n[25]\teval-logloss:0.35958\n[26]\teval-logloss:0.35217\n[27]\teval-logloss:0.34582\n[28]\teval-logloss:0.34494\n[29]\teval-logloss:0.34102\n[30]\teval-logloss:0.33602\n[31]\teval-logloss:0.33534\n[32]\teval-logloss:0.33168\n[33]\teval-logloss:0.32723\n[34]\teval-logloss:0.32658\n[35]\teval-logloss:0.32205\n[36]\teval-logloss:0.31976\n[37]\teval-logloss:0.31801\n[38]\teval-logloss:0.31632\n[39]\teval-logloss:0.31325\n[40]\teval-logloss:0.30951\n[41]\teval-logloss:0.30671\n[42]\teval-logloss:0.30546\n[43]\teval-logloss:0.30190\n[44]\teval-logloss:0.30114\n[45]\teval-logloss:0.30002\n[46]\teval-logloss:0.29690\n[47]\teval-logloss:0.29618\n[48]\teval-logloss:0.29337\n[49]\teval-logloss:0.29213\n[50]\teval-logloss:0.29119\n[51]\teval-logloss:0.28989\n[52]\teval-logloss:0.28954\n[53]\teval-logloss:0.28918\n[54]\teval-logloss:0.28814\n[55]\teval-logloss:0.28649\n[56]\teval-logloss:0.28401\n[57]\teval-logloss:0.28323\n[58]\teval-logloss:0.28172\n[59]\teval-logloss:0.28114\n[60]\teval-logloss:0.28008\n[61]\teval-logloss:0.27815\n[62]\teval-logloss:0.27776\n[63]\teval-logloss:0.27681\n[64]\teval-logloss:0.27617\n[65]\teval-logloss:0.27446\n[66]\teval-logloss:0.27110\n[67]\teval-logloss:0.27057\n[68]\teval-logloss:0.26982\n[69]\teval-logloss:0.26877\n[70]\teval-logloss:0.26854\n[71]\teval-logloss:0.26385\n[72]\teval-logloss:0.26314\n[73]\teval-logloss:0.26252\n[74]\teval-logloss:0.26209\n[75]\teval-logloss:0.26085\n[76]\teval-logloss:0.26027\n[77]\teval-logloss:0.25616\n[78]\teval-logloss:0.25566\n[79]\teval-logloss:0.25556\n[80]\teval-logloss:0.25315\n[81]\teval-logloss:0.25159\n[82]\teval-logloss:0.25146\n[83]\teval-logloss:0.25118\n[84]\teval-logloss:0.24993\n[85]\teval-logloss:0.24818\n[86]\teval-logloss:0.24670\n[87]\teval-logloss:0.24610\n[88]\teval-logloss:0.24557\n[89]\teval-logloss:0.24366\n[90]\teval-logloss:0.24336\n[91]\teval-logloss:0.24222\n[92]\teval-logloss:0.24159\n[93]\teval-logloss:0.24063\n[94]\teval-logloss:0.24057\n[95]\teval-logloss:0.24029\n[96]\teval-logloss:0.23908\n[97]\teval-logloss:0.23888\n[98]\teval-logloss:0.23800\n[99]\teval-logloss:0.23722\nAccuracy: 92.56%\n[0]\teval-logloss:0.65352\n[1]\teval-logloss:0.61931\n[2]\teval-logloss:0.58949\n[3]\teval-logloss:0.56532\n[4]\teval-logloss:0.54421\n[5]\teval-logloss:0.52497\n[6]\teval-logloss:0.50753\n[7]\teval-logloss:0.49291\n[8]\teval-logloss:0.48070\n[9]\teval-logloss:0.46819\n[10]\teval-logloss:0.45718\n[11]\teval-logloss:0.44707\n[12]\teval-logloss:0.43841\n[13]\teval-logloss:0.43072\n[14]\teval-logloss:0.42409\n[15]\teval-logloss:0.41174\n[16]\teval-logloss:0.40552\n[17]\teval-logloss:0.39958\n[18]\teval-logloss:0.38980\n[19]\teval-logloss:0.38484\n[20]\teval-logloss:0.37636\n[21]\teval-logloss:0.37205\n[22]\teval-logloss:0.36725\n[23]\teval-logloss:0.36487\n[24]\teval-logloss:0.36148\n[25]\teval-logloss:0.35958\n[26]\teval-logloss:0.35217\n[27]\teval-logloss:0.34582\n[28]\teval-logloss:0.34494\n[29]\teval-logloss:0.34102\n[30]\teval-logloss:0.33602\n[31]\teval-logloss:0.33534\n[32]\teval-logloss:0.33168\n[33]\teval-logloss:0.32723\n[34]\teval-logloss:0.32658\n[35]\teval-logloss:0.32205\n[36]\teval-logloss:0.31976\n[37]\teval-logloss:0.31801\n[38]\teval-logloss:0.31632\n[39]\teval-logloss:0.31325\n[40]\teval-logloss:0.30951\n[41]\teval-logloss:0.30671\n[42]\teval-logloss:0.30546\n[43]\teval-logloss:0.30190\n[44]\teval-logloss:0.30114\n[45]\teval-logloss:0.30002\n[46]\teval-logloss:0.29690\n[47]\teval-logloss:0.29618\n[48]\teval-logloss:0.29337\n[49]\teval-logloss:0.29213\n[50]\teval-logloss:0.29119\n[51]\teval-logloss:0.28989\n[52]\teval-logloss:0.28954\n[53]\teval-logloss:0.28918\n[54]\teval-logloss:0.28814\n[55]\teval-logloss:0.28649\n[56]\teval-logloss:0.28401\n[57]\teval-logloss:0.28323\n[58]\teval-logloss:0.28172\n[59]\teval-logloss:0.28114\n[60]\teval-logloss:0.28008\n[61]\teval-logloss:0.27815\n[62]\teval-logloss:0.27776\n[63]\teval-logloss:0.27681\n[64]\teval-logloss:0.27617\n[65]\teval-logloss:0.27446\n[66]\teval-logloss:0.27110\n[67]\teval-logloss:0.27057\n[68]\teval-logloss:0.26982\n[69]\teval-logloss:0.26877\n[70]\teval-logloss:0.26854\n[71]\teval-logloss:0.26385\n[72]\teval-logloss:0.26314\n[73]\teval-logloss:0.26252\n[74]\teval-logloss:0.26209\n[75]\teval-logloss:0.26085\n[76]\teval-logloss:0.26027\n[77]\teval-logloss:0.25616\n[78]\teval-logloss:0.25566\n[79]\teval-logloss:0.25556\n[80]\teval-logloss:0.25315\n[81]\teval-logloss:0.25159\n[82]\teval-logloss:0.25146\n[83]\teval-logloss:0.25118\n[84]\teval-logloss:0.24993\n[85]\teval-logloss:0.24818\n[86]\teval-logloss:0.24670\n[87]\teval-logloss:0.24610\n[88]\teval-logloss:0.24557\n[89]\teval-logloss:0.24366\n[90]\teval-logloss:0.24336\n[91]\teval-logloss:0.24222\n[92]\teval-logloss:0.24159\n[93]\teval-logloss:0.24063\n[94]\teval-logloss:0.24057\n[95]\teval-logloss:0.24029\n[96]\teval-logloss:0.23908\n[97]\teval-logloss:0.23888\n[98]\teval-logloss:0.23800\n[99]\teval-logloss:0.23722\nG-Mean: 0.6724\nAUC: 0.7849\nBrier Score: 0.0644\nKS Statistic: 0.4501\nPrecision: 0.1282\nRecall: 0.4839\nF1 Score: 0.2027\nAccuracy: 0.9256\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"\n# 使用np.unique找到所有唯一值及其计数\nunique_values, counts = np.unique(preds_class, return_counts=True)\n\n# 计算唯一值的数量\nunique_count = len(unique_values)\n\nprint(\"唯一值的数量:\", unique_count)\nprint(\"每个唯一值的计数:\")\nfor value, count in zip(unique_values, counts):\n    print(f\"值 {value}: 出现了 {count} 次\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:34.013732Z","iopub.execute_input":"2025-03-04T02:05:34.014093Z","iopub.status.idle":"2025-03-04T02:05:34.022380Z","shell.execute_reply.started":"2025-03-04T02:05:34.014064Z","shell.execute_reply":"2025-03-04T02:05:34.021314Z"}},"outputs":[{"name":"stdout","text":"唯一值的数量: 2\n每个唯一值的计数:\n值 0: 出现了 2936 次\n值 1: 出现了 234 次\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:34.023692Z","iopub.execute_input":"2025-03-04T02:05:34.024039Z","iopub.status.idle":"2025-03-04T02:05:34.049797Z","shell.execute_reply.started":"2025-03-04T02:05:34.024008Z","shell.execute_reply":"2025-03-04T02:05:34.048781Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"label\n0    3108\n1      62\nName: count, dtype: int64"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"## 2.2 Catboost+SMote","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, brier_score_loss\nfrom scipy.stats import ks_2samp\nfrom imblearn.over_sampling import SMOTE\nimport numpy as np\n\n\n# 使用SMOTE对训练集进行过采样\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\nX_train_resampled\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:34.050913Z","iopub.execute_input":"2025-03-04T02:05:34.051196Z","iopub.status.idle":"2025-03-04T02:05:34.109038Z","shell.execute_reply.started":"2025-03-04T02:05:34.051172Z","shell.execute_reply":"2025-03-04T02:05:34.107875Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"        Scode  Year  IndcodeB           TAX            NI            AP  \\\n0        2396  2021        34 -1.305983e+08  8.023291e+08  1.840501e+09   \n1         927  2021        45  3.060950e+08  1.067251e+09  2.397659e+09   \n2         608  2023        63 -1.820569e+07 -2.316339e+08  6.236031e+07   \n3        2040  2010        51  2.172052e+06  2.791069e+07  1.805866e+07   \n4      600010  2020        26  1.914381e+07  6.616602e+08  1.928782e+10   \n...       ...   ...       ...           ...           ...           ...   \n24853  600649  2013        15  8.505210e+06  4.695166e+07  2.683944e+07   \n24854    1846  2017        27  1.385023e+06  1.193035e+07  8.277044e+07   \n24855    1822  2020        42 -2.163949e+09 -1.388335e+10  1.540654e+10   \n24856  138925  2018        42  1.239611e+08  7.558936e+08  4.050087e+08   \n24857  442973  2014        22  1.526348e+06 -3.731567e+08  2.095332e+08   \n\n                 AR            CA            CL           EQU           NCL  \\\n0      1.758021e+09  8.502412e+09  5.283216e+09  5.652276e+09  1.874661e+08   \n1      1.073673e+10  2.525478e+10  1.995466e+10  8.756542e+09  9.965915e+08   \n2      1.260190e+07  1.183622e+08  6.568198e+08  2.917943e+09  7.602333e+08   \n3      1.520956e+07  6.613974e+07  1.795330e+08  5.638807e+08  1.650194e+08   \n4      3.232713e+09  3.913847e+10  5.735439e+10  6.161730e+10  2.525058e+10   \n...             ...           ...           ...           ...           ...   \n24853  3.087543e+07  3.766523e+08  3.159759e+08  3.940750e+08  6.383523e+07   \n24854  1.240974e+08  3.143746e+08  2.082875e+08  4.401777e+08  1.204538e+07   \n24855  2.721991e+09  5.161558e+10  9.121586e+10  1.371279e+10  1.868402e+10   \n24856  5.140333e+08  4.288688e+09  2.595445e+09  2.080209e+09  3.455832e+08   \n24857  4.070418e+08  1.007726e+09  7.456568e+08  1.195537e+09  3.543086e+07   \n\n                 LB     AudOp  EmotionTone1  EmotionTone2  \n0      5.470682e+09  0.000000      0.052000      0.367300  \n1      2.095125e+10  0.000000      0.062500      0.424300  \n2      1.417053e+09  0.000000      0.030500      0.271200  \n3      3.445524e+08  0.000000      0.043200      0.427900  \n4      8.260497e+10  0.000000      0.042200      0.311100  \n...             ...       ...           ...           ...  \n24853  3.798111e+08  0.000000      0.015291      0.095207  \n24854  2.203328e+08  0.247646      0.037810      0.267976  \n24855  1.098999e+11  0.815000      0.023089      0.194345  \n24856  2.941029e+09  0.000000      0.047076      0.382331  \n24857  7.810876e+08  0.736436      0.024020      0.156259  \n\n[24858 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Scode</th>\n      <th>Year</th>\n      <th>IndcodeB</th>\n      <th>TAX</th>\n      <th>NI</th>\n      <th>AP</th>\n      <th>AR</th>\n      <th>CA</th>\n      <th>CL</th>\n      <th>EQU</th>\n      <th>NCL</th>\n      <th>LB</th>\n      <th>AudOp</th>\n      <th>EmotionTone1</th>\n      <th>EmotionTone2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2396</td>\n      <td>2021</td>\n      <td>34</td>\n      <td>-1.305983e+08</td>\n      <td>8.023291e+08</td>\n      <td>1.840501e+09</td>\n      <td>1.758021e+09</td>\n      <td>8.502412e+09</td>\n      <td>5.283216e+09</td>\n      <td>5.652276e+09</td>\n      <td>1.874661e+08</td>\n      <td>5.470682e+09</td>\n      <td>0.000000</td>\n      <td>0.052000</td>\n      <td>0.367300</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>927</td>\n      <td>2021</td>\n      <td>45</td>\n      <td>3.060950e+08</td>\n      <td>1.067251e+09</td>\n      <td>2.397659e+09</td>\n      <td>1.073673e+10</td>\n      <td>2.525478e+10</td>\n      <td>1.995466e+10</td>\n      <td>8.756542e+09</td>\n      <td>9.965915e+08</td>\n      <td>2.095125e+10</td>\n      <td>0.000000</td>\n      <td>0.062500</td>\n      <td>0.424300</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>608</td>\n      <td>2023</td>\n      <td>63</td>\n      <td>-1.820569e+07</td>\n      <td>-2.316339e+08</td>\n      <td>6.236031e+07</td>\n      <td>1.260190e+07</td>\n      <td>1.183622e+08</td>\n      <td>6.568198e+08</td>\n      <td>2.917943e+09</td>\n      <td>7.602333e+08</td>\n      <td>1.417053e+09</td>\n      <td>0.000000</td>\n      <td>0.030500</td>\n      <td>0.271200</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2040</td>\n      <td>2010</td>\n      <td>51</td>\n      <td>2.172052e+06</td>\n      <td>2.791069e+07</td>\n      <td>1.805866e+07</td>\n      <td>1.520956e+07</td>\n      <td>6.613974e+07</td>\n      <td>1.795330e+08</td>\n      <td>5.638807e+08</td>\n      <td>1.650194e+08</td>\n      <td>3.445524e+08</td>\n      <td>0.000000</td>\n      <td>0.043200</td>\n      <td>0.427900</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>600010</td>\n      <td>2020</td>\n      <td>26</td>\n      <td>1.914381e+07</td>\n      <td>6.616602e+08</td>\n      <td>1.928782e+10</td>\n      <td>3.232713e+09</td>\n      <td>3.913847e+10</td>\n      <td>5.735439e+10</td>\n      <td>6.161730e+10</td>\n      <td>2.525058e+10</td>\n      <td>8.260497e+10</td>\n      <td>0.000000</td>\n      <td>0.042200</td>\n      <td>0.311100</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24853</th>\n      <td>600649</td>\n      <td>2013</td>\n      <td>15</td>\n      <td>8.505210e+06</td>\n      <td>4.695166e+07</td>\n      <td>2.683944e+07</td>\n      <td>3.087543e+07</td>\n      <td>3.766523e+08</td>\n      <td>3.159759e+08</td>\n      <td>3.940750e+08</td>\n      <td>6.383523e+07</td>\n      <td>3.798111e+08</td>\n      <td>0.000000</td>\n      <td>0.015291</td>\n      <td>0.095207</td>\n    </tr>\n    <tr>\n      <th>24854</th>\n      <td>1846</td>\n      <td>2017</td>\n      <td>27</td>\n      <td>1.385023e+06</td>\n      <td>1.193035e+07</td>\n      <td>8.277044e+07</td>\n      <td>1.240974e+08</td>\n      <td>3.143746e+08</td>\n      <td>2.082875e+08</td>\n      <td>4.401777e+08</td>\n      <td>1.204538e+07</td>\n      <td>2.203328e+08</td>\n      <td>0.247646</td>\n      <td>0.037810</td>\n      <td>0.267976</td>\n    </tr>\n    <tr>\n      <th>24855</th>\n      <td>1822</td>\n      <td>2020</td>\n      <td>42</td>\n      <td>-2.163949e+09</td>\n      <td>-1.388335e+10</td>\n      <td>1.540654e+10</td>\n      <td>2.721991e+09</td>\n      <td>5.161558e+10</td>\n      <td>9.121586e+10</td>\n      <td>1.371279e+10</td>\n      <td>1.868402e+10</td>\n      <td>1.098999e+11</td>\n      <td>0.815000</td>\n      <td>0.023089</td>\n      <td>0.194345</td>\n    </tr>\n    <tr>\n      <th>24856</th>\n      <td>138925</td>\n      <td>2018</td>\n      <td>42</td>\n      <td>1.239611e+08</td>\n      <td>7.558936e+08</td>\n      <td>4.050087e+08</td>\n      <td>5.140333e+08</td>\n      <td>4.288688e+09</td>\n      <td>2.595445e+09</td>\n      <td>2.080209e+09</td>\n      <td>3.455832e+08</td>\n      <td>2.941029e+09</td>\n      <td>0.000000</td>\n      <td>0.047076</td>\n      <td>0.382331</td>\n    </tr>\n    <tr>\n      <th>24857</th>\n      <td>442973</td>\n      <td>2014</td>\n      <td>22</td>\n      <td>1.526348e+06</td>\n      <td>-3.731567e+08</td>\n      <td>2.095332e+08</td>\n      <td>4.070418e+08</td>\n      <td>1.007726e+09</td>\n      <td>7.456568e+08</td>\n      <td>1.195537e+09</td>\n      <td>3.543086e+07</td>\n      <td>7.810876e+08</td>\n      <td>0.736436</td>\n      <td>0.024020</td>\n      <td>0.156259</td>\n    </tr>\n  </tbody>\n</table>\n<p>24858 rows × 15 columns</p>\n</div>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"\n# 指定哪些特征是类别型的（在这个例子中，IndcodeB是）\ncat_features_indices = []\n# 创建CatBoost数据池\ntrain_pool = Pool(data=X_train_resampled, label=y_train_resampled, cat_features=cat_features_indices)\ntest_pool = Pool(data=X_test, label=y_test, cat_features=cat_features_indices)\n\n# 设置CatBoost参数\nparams = {\n    'iterations': 100,\n    'learning_rate': 0.1,\n    'depth': 6,\n    'loss_function': 'Logloss',  # 对于二分类问题\n    'eval_metric': 'AUC',\n    'random_seed': 42,\n    'verbose': 10  # 每10次迭代打印一次日志\n}\n\n# 训练模型\nmodel = CatBoostClassifier(**params)\nmodel.fit(train_pool, eval_set=test_pool, early_stopping_rounds=20)\n\n# 在测试集上进行预测\npreds_proba = model.predict_proba(test_pool)[:, 1]  # 获取正类的概率\npreds_class = model.predict(test_pool)  # 获取预测类别\n\n# 计算评估指标\nauc = roc_auc_score(y_test, preds_proba)\nbrier = brier_score_loss(y_test, preds_proba)\nks_stat, _ = ks_2samp(preds_proba[y_test == 1], preds_proba[y_test == 0])\nprecision = precision_score(y_test, preds_class)\nrecall = recall_score(y_test, preds_class)\nf1 = f1_score(y_test, preds_class)\naccuracy = accuracy_score(y_test, preds_class)\n\n# 计算G-Mean\nsensitivity = recall_score(y_test, preds_class, pos_label=1)  # 灵敏度（召回率）\nspecificity = recall_score(y_test, preds_class, pos_label=0)  # 特异度\ngmean = np.sqrt(sensitivity * specificity)\n\n# 打印所有评估指标\nprint(f\"G-Mean: {gmean:.4f}\")\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"Brier Score: {brier:.4f}\")\nprint(f\"KS Statistic: {ks_stat:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:34.110150Z","iopub.execute_input":"2025-03-04T02:05:34.110644Z","iopub.status.idle":"2025-03-04T02:05:34.706184Z","shell.execute_reply.started":"2025-03-04T02:05:34.110601Z","shell.execute_reply":"2025-03-04T02:05:34.705137Z"}},"outputs":[{"name":"stdout","text":"0:\ttest: 0.7627377\tbest: 0.7627377 (0)\ttotal: 6.25ms\tremaining: 619ms\n10:\ttest: 0.7817651\tbest: 0.7833946 (8)\ttotal: 53.3ms\tremaining: 431ms\n20:\ttest: 0.8132369\tbest: 0.8132369 (20)\ttotal: 105ms\tremaining: 397ms\n30:\ttest: 0.8169474\tbest: 0.8169474 (30)\ttotal: 153ms\tremaining: 341ms\n40:\ttest: 0.8163584\tbest: 0.8194773 (37)\ttotal: 201ms\tremaining: 289ms\n50:\ttest: 0.8188442\tbest: 0.8211847 (45)\ttotal: 249ms\tremaining: 240ms\n60:\ttest: 0.8183564\tbest: 0.8211847 (45)\ttotal: 298ms\tremaining: 191ms\n70:\ttest: 0.8264209\tbest: 0.8264209 (70)\ttotal: 348ms\tremaining: 142ms\n80:\ttest: 0.8286524\tbest: 0.8291194 (79)\ttotal: 396ms\tremaining: 92.8ms\n90:\ttest: 0.8269865\tbest: 0.8291194 (79)\ttotal: 442ms\tremaining: 43.7ms\n99:\ttest: 0.8206346\tbest: 0.8291194 (79)\ttotal: 486ms\tremaining: 0us\n\nbestTest = 0.829119442\nbestIteration = 79\n\nShrink model to first 80 iterations.\nG-Mean: 0.6973\nAUC: 0.8291\nBrier Score: 0.0597\nKS Statistic: 0.5343\nPrecision: 0.1509\nRecall: 0.5161\nF1 Score: 0.2336\nAccuracy: 0.9338\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"\n# 使用np.unique找到所有唯一值及其计数\nunique_values, counts = np.unique(preds_class, return_counts=True)\n\n# 计算唯一值的数量\nunique_count = len(unique_values)\n\nprint(\"唯一值的数量:\", unique_count)\nprint(\"每个唯一值的计数:\")\nfor value, count in zip(unique_values, counts):\n    print(f\"值 {value}: 出现了 {count} 次\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:34.707120Z","iopub.execute_input":"2025-03-04T02:05:34.707451Z","iopub.status.idle":"2025-03-04T02:05:34.715573Z","shell.execute_reply.started":"2025-03-04T02:05:34.707382Z","shell.execute_reply":"2025-03-04T02:05:34.714607Z"}},"outputs":[{"name":"stdout","text":"唯一值的数量: 2\n每个唯一值的计数:\n值 0: 出现了 2958 次\n值 1: 出现了 212 次\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## 2.3 NN+SMote","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:34.716716Z","iopub.execute_input":"2025-03-04T02:05:34.717099Z","iopub.status.idle":"2025-03-04T02:05:34.744131Z","shell.execute_reply.started":"2025-03-04T02:05:34.717063Z","shell.execute_reply":"2025-03-04T02:05:34.742830Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# 使用SMOTE对训练集进行过采样\nsmote = SMOTE(random_state=42)\nX_train_resampled_scaled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\nX_train_resampled_scaled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:34.745246Z","iopub.execute_input":"2025-03-04T02:05:34.745612Z","iopub.status.idle":"2025-03-04T02:05:34.770268Z","shell.execute_reply.started":"2025-03-04T02:05:34.745580Z","shell.execute_reply":"2025-03-04T02:05:34.769216Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"array([[-0.98278826, -1.61245155, -0.71168241, ..., -0.13426574,\n        -0.4843976 , -0.47797305],\n       [-0.98907434,  0.12403473,  2.5393298 , ..., -0.13426574,\n         0.92210821,  0.99990884],\n       [-0.98349056,  1.61245155,  1.80325156, ..., -0.13426574,\n        -1.1142676 , -0.74240197],\n       ...,\n       [-0.98776129,  0.80767459, -0.74306882, ..., -0.13426574,\n        -2.0220101 , -2.1226662 ],\n       [-0.98719305,  0.06282944, -1.54945826, ..., -0.13426574,\n        -1.14021488, -1.20732016],\n       [-0.98825431,  0.75108887, -0.12044718, ..., -0.13426574,\n        -3.47183569, -3.71290952]])"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"# 构建模型\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))  # 输入层+第一隐藏层\nmodel.add(Dense(16, activation='relu'))  # 第二隐藏层\nmodel.add(Dense(1, activation='sigmoid'))  # 输出层，如果是多分类请使用softmax\n\n# 编译模型\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# 训练模型\nhistory = model.fit(X_train_resampled_scaled, y_train_resampled, epochs=30, batch_size=10, validation_data=(X_test, y_test))\n\n# 评估模型\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:05:34.771236Z","iopub.execute_input":"2025-03-04T02:05:34.771600Z","iopub.status.idle":"2025-03-04T02:07:38.252858Z","shell.execute_reply.started":"2025-03-04T02:05:34.771568Z","shell.execute_reply":"2025-03-04T02:07:38.251702Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7324 - loss: 0.5291 - val_accuracy: 0.7946 - val_loss: 0.4078\nEpoch 2/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8278 - loss: 0.3928 - val_accuracy: 0.8410 - val_loss: 0.3530\nEpoch 3/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8463 - loss: 0.3570 - val_accuracy: 0.8170 - val_loss: 0.3954\nEpoch 4/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8579 - loss: 0.3354 - val_accuracy: 0.8467 - val_loss: 0.3302\nEpoch 5/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8696 - loss: 0.3126 - val_accuracy: 0.8435 - val_loss: 0.3549\nEpoch 6/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8817 - loss: 0.2938 - val_accuracy: 0.8577 - val_loss: 0.3098\nEpoch 7/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8811 - loss: 0.2909 - val_accuracy: 0.8603 - val_loss: 0.3162\nEpoch 8/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8941 - loss: 0.2646 - val_accuracy: 0.8483 - val_loss: 0.3272\nEpoch 9/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8980 - loss: 0.2606 - val_accuracy: 0.8486 - val_loss: 0.3370\nEpoch 10/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8963 - loss: 0.2561 - val_accuracy: 0.8713 - val_loss: 0.2960\nEpoch 11/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9043 - loss: 0.2446 - val_accuracy: 0.8577 - val_loss: 0.3180\nEpoch 12/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9049 - loss: 0.2396 - val_accuracy: 0.8237 - val_loss: 0.4051\nEpoch 13/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9102 - loss: 0.2320 - val_accuracy: 0.8590 - val_loss: 0.3140\nEpoch 14/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9128 - loss: 0.2270 - val_accuracy: 0.8836 - val_loss: 0.2855\nEpoch 15/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9112 - loss: 0.2248 - val_accuracy: 0.8899 - val_loss: 0.2567\nEpoch 16/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9207 - loss: 0.2117 - val_accuracy: 0.8710 - val_loss: 0.3098\nEpoch 17/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9223 - loss: 0.2080 - val_accuracy: 0.8448 - val_loss: 0.3780\nEpoch 18/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9205 - loss: 0.2091 - val_accuracy: 0.8937 - val_loss: 0.2787\nEpoch 19/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9254 - loss: 0.2011 - val_accuracy: 0.8798 - val_loss: 0.3039\nEpoch 20/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9224 - loss: 0.2035 - val_accuracy: 0.8666 - val_loss: 0.3332\nEpoch 21/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9270 - loss: 0.1968 - val_accuracy: 0.8918 - val_loss: 0.2823\nEpoch 22/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9266 - loss: 0.1937 - val_accuracy: 0.8808 - val_loss: 0.3093\nEpoch 23/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9278 - loss: 0.1965 - val_accuracy: 0.8804 - val_loss: 0.3090\nEpoch 24/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9286 - loss: 0.1890 - val_accuracy: 0.8823 - val_loss: 0.3139\nEpoch 25/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9293 - loss: 0.1913 - val_accuracy: 0.8871 - val_loss: 0.3032\nEpoch 26/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9312 - loss: 0.1868 - val_accuracy: 0.9054 - val_loss: 0.2682\nEpoch 27/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9319 - loss: 0.1889 - val_accuracy: 0.9132 - val_loss: 0.2519\nEpoch 28/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9298 - loss: 0.1847 - val_accuracy: 0.8946 - val_loss: 0.2982\nEpoch 29/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9348 - loss: 0.1803 - val_accuracy: 0.8707 - val_loss: 0.3480\nEpoch 30/30\n\u001b[1m2489/2489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9349 - loss: 0.1777 - val_accuracy: 0.9151 - val_loss: 0.2583\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9192 - loss: 0.2653\nTest Accuracy: 0.92\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# 在测试集上进行预测\npreds_proba = model.predict(X_test)  # 获取正类的概率\npreds_class = (model.predict(X_test) > 0.5).astype(\"int32\")  # 获取预测类别\n\n# 计算评估指标\nauc = roc_auc_score(y_test, preds_proba)\nbrier = brier_score_loss(y_test, preds_proba)\nks_stat, _ = ks_2samp(preds_proba[y_test == 1].ravel(), preds_proba[y_test == 0].ravel())\nprecision = precision_score(y_test, preds_class)\nrecall = recall_score(y_test, preds_class)\nf1 = f1_score(y_test, preds_class)\naccuracy = accuracy_score(y_test, preds_class)\n\n# 计算G-Mean\nsensitivity = recall_score(y_test, preds_class, pos_label=1)  # 灵敏度（召回率）\nspecificity = recall_score(y_test, preds_class, pos_label=0)  # 特异度\ngmean = np.sqrt(sensitivity * specificity)\n\n# 打印所有评估指标\nprint(f\"G-Mean: {gmean:.3f}\")\nprint(f\"AUC: {auc:.3f}\")\nprint(f\"Brier Score: {brier:.3f}\")\nprint(f\"KS Statistic: {ks_stat:.3f}\")\nprint(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\nprint(f\"F1 Score: {f1:.3f}\")\nprint(f\"Accuracy: {accuracy:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T02:07:38.253879Z","iopub.execute_input":"2025-03-04T02:07:38.254188Z","iopub.status.idle":"2025-03-04T02:07:38.824021Z","shell.execute_reply.started":"2025-03-04T02:07:38.254161Z","shell.execute_reply":"2025-03-04T02:07:38.822841Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\nG-Mean: 0.689\nAUC: 0.817\nBrier Score: 0.064\nKS Statistic: 0.516\nPrecision: 0.140\nRecall: 0.514\nF1 Score: 0.220\nAccuracy: 0.915\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}